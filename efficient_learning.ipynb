{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPGsTuS5/5JOhV0WlbA7OcS"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Efficient Deep Neural Networks\n",
        "AI is getting super smart and impressive but the model size is increasing exponentially either in recent years. Model size is basically the number of parameters in the models but another important concept is the memory which is much more expensive than computation. For example, a multiplication operation requires 3.7 pico joules but accessing the DRAM memory requires 640  pico joules which is significantly higher. Hence, more data movement means we need to do more memory reference which will lead to a higher amount of energy consumption. Eventually, data movement from memory drains the battery of our mobile devices so we want to make the memory movement as little as possible.\n",
        "\n",
        "In order to do that we can reduce the model size, activation size and the workload. Then, from the systems perspective, we can build more efficient hardware or better compilers with better scheduling policy to encourage more locality to reduce data movement. Here, we will talk about the first method which is more of an algorithm perspective to fundamentally reduce the requirement for data movement. It generally includes; pruning, quantization, knowledge distilation, and more recently NAS (Neural Architecture Search).\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/efficient_learning/ai_is_too_big.png?raw=true' width=650 >\n"
      ],
      "metadata": {
        "id": "DTkApFC3X1Pr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning\n",
        "Pruning naturally happens in human brain. A newborn child has about 2500 synapses per neuron and when the child becomes a toddler this number surges very quickly to 15000 synapses per neuron. However when he or she gets to adolescence this number didn't keep increasing but started to decrease until to adulthood where we have roughly 7000 synapses per neuron it's still more than a newborn child but it's definitely half times smaller than a toddler.\n",
        "So, pruning naturally happens in the human brain when we are developing our brain. Those important connections get capped and unimportant synapses gets pruned away. \n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/efficient_learning/pruning_inspiration.png?raw=true' width=650 >\n",
        "\n",
        "Inspired by that, we can make neural networks smaller by removing some of the connections and some of the neurons. But, how exactly are we going to do pruning? Intuitively, we first train the model to convergence and recognize which neurons are important and which are not. Then, we are going to prune away some of those unimportant connections and train rest of the network. Actually, we can continue this process iteratively: prune some parameters, retrain the remaining. At the end, we can remove 90 percent of AlexNet without losing any accuracy which is wonderful. However, if we would repeat the same processes without retraining, we would not be able to reach same accuracy level since we are interfere the weight distribution. Briefly, this is idea of pruning the neural networks.\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/efficient_learning/iterative_pruning.png?raw=true' width=550 >\n"
      ],
      "metadata": {
        "id": "IzCMuxlwhB_C"
      }
    }
  ]
}
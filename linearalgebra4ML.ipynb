{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LinearAlgebra4ML.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQa59XQaNgau7Y/KRUd9hK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Linear Algebra for Machine Learning\n",
        "\n",
        "Linear algebra is a mathematical discipline that deals with vectors and matrices and, more generally, with vector spaces and linear transformations. This was a formal definition of a linear algebra but lets go over an example to understand it clearly: \n",
        "\n",
        "We went for a shopping on two occasions and bought apples and bananas: In the first shopping, we bought 2 apples and 3 bananas and, they cost 8 Euros. In the second time, however, we got 10 apples and 1 banana, and the cost was 13 Euros. In order to discover the price of the apple and banana,  what we going to do is solving these equations simultaneously:\n",
        "\n",
        "$2a + 3b = 8$\n",
        "\n",
        "$10a + b = 13$ \n",
        "\n",
        "If it looks easy, in general, there are lots of different types of items and lots of shopping trips, then finding out the prices might be quite hard. Hence, it might be quite difficult to solve all these equations by hand. Now, this is an example of a Linear Algebra problem. "
      ],
      "metadata": {
        "id": "L11ZJwei4aKs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another type of problem that linear algebra tries to solve is optimization of a function that represents some data based on some parameters. This kind of problems are also associated with multivariate calculus.\n",
        "\n",
        "Lets say we wanted to fit a function for height of a population. And let's say that equation has just two parameters. One parameter that called $μ$ describes the center of the distribution. And other one describes how wide the distribution is, called $σ$. This is called the Normal or Gaussian distribution. And it's got a center of $μ$ and a width of $σ$. And it's normalized, so that it's area is equal to 1. Now, we could fit with some curve that had two parameters, $μ$ and $σ$ and our aim should be finding best possible $μ$ and $σ$ which represent the data very well.\n",
        "\n",
        "However, imagine that we had guessed that the width was wider than it really is, but keeping the area of 1. So our function's distribution would be fatter and shorter. If we add up differences between measurements and your estimates in magnitude (places our function underestimate or overestimate), the value we get will be *goodness of fit*. \n",
        "\n",
        "To spice things up, we could imagine plotting out all of the possible values of $μ$ and $σ$ with respect to *goodness of fit*.  On the plot, if we do a little move, does it get better or worse? If it gets better, we should keep moving in that direction right? Based on that logic, the steepest way down the hill based on the set of contours, would be the best path to follow so that we can reach the optimal $μ$ and $σ$ values faster. Actually, in that scenerio, vectors are just moving around the space. They're not moving around a physical space but they're moving around a parameter space. To do this well, we'll want to understand how to work with vectors and then how to do calculus on those vectors in order to find gradients in these contour maps. Finally, we'll be able to do optimization which enables to work with data and do machine learning and data science.\n",
        "\n",
        "Similarly, we could write down a vector that represents all of the things about a car such as its cost, emission, Euro NCAP star rating and top speed. Writing all those features down creates a vector. Again, we can try to find the best function that represents the distribution of the real data along with the space. Actually, this was what Einstein was did back in time: he conceived time as just being another dimension that is why we called the time as a fourth dimension, three dimension of metres, and one dimension of time in seconds."
      ],
      "metadata": {
        "id": "KauXhw5i8jgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To define a vector in the n-dimensional space,  what we just need is n-independent vectors that represent each dimension so that any vector can be seen as a combination of these n-independent vectors.\n",
        "\n",
        "Let's define a space by 2-independent vectors: we will have 2-dimensional space. We can call the first vector ⅈ which has a unit length and takes us from left to right. Other vector that goes up can be called as a vector ⅉ which also,have a unit length, length of one. From now on, we can define any vector we want by using these 2 independent vectors in the 2 dimensional space."
      ],
      "metadata": {
        "id": "yAjTZFlIGnwQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modulus, Dot Product, Projection and Changing the Basis \n",
        "Lets define 2 independent unit vectors: vector ⅈ [*1,0*] and vector ⅉ [*0,1*]. We can now define another vector $r$ based on those two vectors:\n",
        "\n",
        "$ai + bj = r= \\begin{bmatrix}a \\\\ b \\end{bmatrix} $where $a$ and $b$ are coefficients for vector ⅈ and vector ⅉ and $sqrt(a^2 + b^2)$ gives the size of $r$.\n",
        "\n",
        "If we have two vectors, $r$ and $s$, summing two vector just an element-wise addition. Similarly, dot product of two vector is just **a sum of** element-wise multiplication. For example, $r = \\begin{bmatrix} ri \\\\ rj \\end{bmatrix}$ and $s = \\begin{bmatrix} si \\\\ sj \\end{bmatrix}$ and $r.s$ would be equal to $ri.si + rj.sj$.\n",
        "\n",
        "Interestingly, there is a link between the dot product and the length or modulus of a vector. If we take a vector and dot it with itself, what we get is the sums of the squares of its components. That's fascinating because that means if we take the dot product of a vector with itself, we will get the square of its modulus:\n",
        "\n",
        "$r.r = ri.ri +rj.rj = ri^2 +rj^2 = |r|^2 $ where $r = \\begin{bmatrix} ri \\\\ rj \\end{bmatrix}$ \n",
        "\n",
        "So, if we want to get the size of a vector, we can do that just by dotting the vector with itself and taking the square root. That is really neat and satisfying.\n",
        "\n",
        "To dig deeper in dot product, we can visit the cosine rule: $x^2= r^2 + s^2 - 2rs.cosθ$ where $c = r - s$,\n",
        "\n",
        "Hence, it is actually equal to $|r - s|^2= |r|^2 + |s|^2 - 2rs.cosθ$,\n",
        "\n",
        "From the *modulus rule*; $|r - s|^2$ will be equal to $(r - s).(r - s) = r.r -2rs + s.s = |r|^2 -2rs + |s|^2$,\n",
        "\n",
        "Our equation now become $|r|^2 -2rs + |s|^2 = |r|^2 + |s|^2 - 2rs.cosθ$ and it is finally equal to $r.s = |r||s|cosθ$.\n",
        "\n",
        "The intuition here is when the degree between two vectors is 90, the dot product will equal to 0. When they go in the same way or the degree between them smaller than 90, dot product will get a positive answer. In contrast, when they're going in opposite directions dot product will get a negative answer.\n",
        "\n",
        "Projection is also closely related with dot product and cosine rule. We know that $cosθ = adjacent/hypotenuse$ and let hypotenuse is equal to $vector s$. In that case, our $adjacent$ will be equal to projection of $vector s$ onto $r$:\n",
        "\n",
        "$cosθ$ = $adj \\over hyp$ = $adj \\over |s|$.\n",
        "\n",
        "If we do some math, we can easliy see that $adj=|s|cosθ$ where $adj$ is equal to projection. And, we already know that $r.s = |r||s|cosθ$.\n",
        "\n",
        "Finally, if we substitute $|s|cosθ$ with $adj$ or projection our equation will be somthing like that:\n",
        "\n",
        "$r.s = |r|.projection$ → $projection$ = $r.s \\over |r|$ which is called a *scalar projection*. That is why dot product also known as projection product.\n",
        "\n",
        "But what about *vector projection*? To do the vector projection we can use the following equation:\n",
        "\n",
        "$r.s \\over|r||r|$$r$ = $r.s \\over|r|^2$$r$ = $r.s \\over r.r$$r$\n",
        "\n",
        "Here, $r.s \\over r.r$ returns only a scalar value and $r$ is our basis vector that we project $s$ onto. So, the intuition is how much or what amount of vector $r$ is contained in vector $s$. \n",
        "\n"
      ],
      "metadata": {
        "id": "4z3T_uVcgqeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The projection of a vector to another can be used for changing the basis or defining an alternative new coordinate system. Lets say currently we are defining our 2d space with 2 independent unit vectors ⅈ and ⅉ which means all the vectors in that space represented by these two vectors. And in that space, we can have tons of vectors just like vector $r$, vector $s$ and vector $k$. We learnt that vector projection gives an intuition about how much one vector contains another vector. Hence, instead of representing vectors $r$, $s$ and $k$ by vectors ⅈ and ⅉ, now we can represent them based on orthogonal vectors $r$ and $s$, for example.\n",
        "\n",
        "$r = \\begin{bmatrix} ri \\\\ rj \\end{bmatrix}$,\n",
        "$s = \\begin{bmatrix} si \\\\ sj \\end{bmatrix}$,\n",
        "$k = \\begin{bmatrix} ki \\\\ kj \\end{bmatrix}$ which have basis ⅈ and ⅉ.\n",
        "\n",
        "$k = \\begin{bmatrix} kr \\\\ ks \\end{bmatrix}$ which have basis $r$ and $s$ by using our projection equation: $r.k \\over r.r$$r$ and $s.k \\over s.s$$s$ \n",
        "\n",
        "\n",
        "This is amazing because the vector $k$ has some existence completely independently of the coordinate system we use to describe the numbers. All the vectors that take us away from the origin will still exist, independently of the numbers we used in $k$. This is neat.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZUrdyXhAwM9J"
      }
    }
  ]
}
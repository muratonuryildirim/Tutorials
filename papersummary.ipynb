{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "papersummary.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNTETtD15mdmqM8DKwcCH51"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science (SET)  \n",
        "##*Nature Communications 2018*\n",
        "\n",
        "Randomly initialize SCLs in our network and start training. \n",
        "\n",
        "At the end of each epoch, we remove the connections with the smallest weights (the “weakest” connections) based on a threshold $t$ and replace them with randomly initialized new ones. \n",
        "\n",
        "Repeat.\n",
        "\n",
        "SET turns out to be surprisingly robust and stable. Encouragingly, the authors are able to show very similar results to FCL models (sometimes surpassing their performance) with SET models that contain far fewer parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lCf1YYM9eev7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization (Dynamic Sparse Reparameterization - DSR)\n",
        "##*ICML 2019*\n",
        "\n",
        "Randomly initialize SCLs in our network and start training.\n",
        "\n",
        "Calculate mean magnitude of momentum $G$ for each layer .\n",
        "\n",
        "Remove connections with smallest weights based on global adaptive threshold $t$.\n",
        "\n",
        "Immediately after removing $K$ number of parameters during the pruning phase, $K$ zero-initialized parameters are redistributed among the sparse parameter tensors, based on calculated mean magnitude of gradient $G$: layers having larger fractions of non-zero weights receive proportionally more free parameters. This means, free parameters should be redistributed to layers whose parameters receive larger loss gradients.\n",
        "\n",
        "Repeat.\n"
      ],
      "metadata": {
        "id": "VW1HUmK7eezq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Networks from Scratch: Faster Training without Losing Performance (Sparse Momentum - SM)\n",
        "##*ICLR 2020*\n",
        "\n",
        "Randomly initialize SCLs in our network and start training.\n",
        "\n",
        "Calculate mean magnitude of momentum $M$ for each layer .\n",
        "\n",
        "Remove the smallest 50% of weights for each layer.\n",
        "\n",
        "Immediately after removing $K$ number of parameters during the pruning phase, $K$ zero-initialized parameters are redistributed among the sparse parameter tensors, based on calculated mean magnitude of momentum $M$: layers having larger fractions of mean momentum will receive proportionally more free parameters.\n",
        "\n",
        "Repeat.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/SM.jpg?raw=true\" width=700>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "gxboUvv9ee6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers (DST)\n",
        "##*ICLR 2020*\n",
        "\n",
        "Randomly initialize Fully Connected Neural Network.\n",
        "\n",
        "Randomly initialize trainable mask layers (layer-level threshold $t$) and start training on Masked Neural Network.\n",
        "\n",
        "Instead of pruning (masking) between two training epochs with a predefined pruning schedule, this method prunes and recovers the network parameters at each training step, which is far more fine-grained than existing methods.\n",
        "\n",
        "In each training step, parameter is subtracted with respective threshold value and it is going to be masked if the values is smaller than 0. Not masked or pruned, otherwise:\n",
        "\n",
        "$Q$<sub>ij</sub>= $F$($W$<sub>ij</sub> ,$t$<sub>i</sub>) = $|W$<sub>ij</sub>$|$ - $t$<sub>i</sub>\n",
        "\n",
        "$M$<sub>ij</sub> = $S(Q$<sub>ij</sub>$)$ where $M$<sub>ij</sub>$= 1$ if not pruned, $M$<sub>ij</sub> $= 0$ if pruned\n",
        "\n",
        "Repeat.\n",
        "\n",
        "However, authors realize that $t$<sub>i</sub> cannot be learnt or updated under this funtions $S(x)$ since its gradient is always equal to 0. Therefore, they come up with  approximation funtion $H(x)$ which allows to learn ti and consequenly all the mask layers since it has a gradient.\n",
        "\n",
        "Finally, after training, the model would be sparse based on trained mask layers (layer-level threshold $t$<sub>i</sub>).\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/DST.jpg?raw=true\" width=550>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ORhOuA9See8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\n",
        "##*CVPR 2018*\n",
        "#### Arun Mallya and Svetlana Lazebnik\n",
        "\n",
        "Inspired by network pruning techniques, PackNet exploits redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, PackNet is able to sequentially “pack” multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. To do that, after finding $n$<sup>th</sup> pack, it freezes and removes that \"pack\" from the large backbone. Hence, $n+1$<sup>th</sup> pack that will be constructed for the next task will not interfere the $n$<sup>th</sup> pack and it repeats same steps for the rest of the tasks to avoid catastrophic forgetting. In the figure, white circles represents available neurons in the backbone while bold circles indicates neurons that are already occupied in another pack that is why in the next pack selection these neurons will be discarded.\n",
        "\n",
        "* **Masking Method**: Train the all backbone then remove 50% or 75% of the connections based on weights' absolute magnitude. Re-train.\n",
        "* **Mask Selection**: -\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/packnet.png?raw=true\" width=800>\n",
        "</p>"
      ],
      "metadata": {
        "id": "jM5KlF8g1zLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights\n",
        "## *ECCV 2018*\n",
        "#### Arun Mallya, Dillon Davis, and Svetlana Lazebnik\n",
        "\n",
        "Inspired by PackNET which adopts a different route by iteratively pruning unimportant weights and fine-tuning them for learning new tasks. \n",
        "Piggyback questions whether the weights of a network have to be changed at all. It suggest we might get reasonable results with just selectively masking, or setting certain weights to 0, while keeping the rest of the weights the same as before. \n",
        "\n",
        "Based on this idea, Piggyback learns how to mask weights of an existing “backbone” network for obtaining good performance on a new task, as shown in the Figure. Binary masks that take values in {0, 1} are learned and stored after each task. This simple idea is mostly suitable for task-IL scenario.\n",
        "\n",
        "* **Masking Method**:\n",
        "* **Mask Selection**: -\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/piggyback.png?raw=true\" width=700>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "DK7rElqguenW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SupSup: Supermasks in Superposition\n",
        "## *NeurIPS 2020*\n",
        "#### Mitchell Wortsman et al.\n",
        "\n",
        "Supermasks in Superposition (SupSup) model uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask). If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. Authors experimentally find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. Hence, SupSup is suitable for class-IL as well.\n",
        "\n",
        "\n",
        "During training, SupSup learns a separate supermask (subnetwork) for each task. At inference time, SupSup can infer task identity by superimposing all supermasks. Ideally, appropriate supermask for a given task should exhibit a confident output distribution (i.e. low entropy).\n",
        "\n",
        "* **Masking Method**: Deconstructing lottery tickets: Zeros, signs, and the supermask.\n",
        "* **Mask Selection**: Try all the supermasks, return mask with a lowest entropy\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/supsup.png?raw=true\" width=800>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ItpNEe7__RUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpaceNet: Make Free Space for Continual Learning\n",
        "## *Neurocomputing 2021*\n",
        "#### Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy\n",
        "\n",
        "SpaceNet trains sparse deep neural networks from scratch to have compact number of neurons for each task. The adaptive training of the sparse connections results in sparse representations per task that reduce the interference time. Experimental results show the robustness of the method against catastrophic forgetting and leaving space for more tasks to be learned. \n",
        "\n",
        "When the model faces a new task, new sparse connections are randomly allocated between a selected number of neurons in each layer. At the end of the training, the initial distribution of the connections changes and connections that are important for that task group together.\n",
        "The most important neurons for a specific task are reserved to be specific to this task, and will not be seen by the following tasks and will freeze. However, other neurons that are somehow important or not important at all will continue to be shared between the tasks. For example, in the figure, fully filled circles represent the neurons that are most important and become specific for $task$ $t$, where partially filled ones are less important and could be shared by other tasks. Multiple colored circles represent the neurons that are used by multiple tasks. After learning $task$ $t$, the corresponding weights are kept fixed.\n",
        "\n",
        "For convolutional neural networks, SpaceNet performs a coarse manner in drop and grow phases to impose structure sparsity instead of irregular sparsity. In particular, in the drop phase, coarse removal for the whole kernel is applied instead of removing scalar weights. Similarly, in the grow phase, the whole connections of a kernel are added instead of adding single weights. The likelihood of adding a kernel between two feature maps is inversely proportional to their significance, similar to multilayer perceptron networks. The feature map's importance is determined by adding the importance of its connected kernels.\n",
        "\n",
        "* **Masking Method**: A fraction $r$ of the sparse connections in each layer is dynamically changed based on the importance of the connections and neurons in that layer. Connection importance is estimated by its contribution to the change in the loss function. The first-order Taylor approximation is used to approximate the change in loss during one training iteration $i$. Growing and Dropping the connection is applied based on the importance score.\n",
        "\n",
        "* **Mask Selection**: -\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/spacenet.png?raw=true\" width=700>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "0W2HuqQfIoJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Prune-and-Select (CP&S): Class-Incremental Learning with specialized subnetworks\n",
        "##*2022*\n",
        "#### Aleksandr Dekhovich, David M.J. Tax, Marcel H.F. Sluiter, and Miguel A. Bessa\n",
        "\n",
        "During training, Continual-Prune-and-Select (CP&S) finds a subnetwork within the DNN that is responsible for solving a given $task$ $t$. \n",
        "\n",
        "A new task is learned by training available neuronal connections (previously untrained) of the DNN to create a new subnetwork which can include previously trained connections belonging to other subnetwork(s) but those will not be updated. In other words, previously trained connections can be shared between tasks yet cannot be updated.\n",
        "\n",
        "Then, during inference, CP&S selects the correct subnetwork to make predictions for that task.  This enables to eliminate catastrophic forgetting by creating specialized regions in the DNN that do not conflict with each other while still allowing knowledge transfer across them. \n",
        "\n",
        "\n",
        "* **Masking Method**: NNrelief\n",
        "* **Mask Selection**: Try all the subnetworks, return mask with a lowest entropy (*maximum output response*)"
      ],
      "metadata": {
        "id": "y40AC2v9facf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forget-free Continual Learning with Winning Subnetworks\n",
        "##*ICML 2022*\n",
        "#### Haeyong Kang et al.\n",
        "\n",
        "For each task, the WSN sequentially learns and chooses the best subnetwork. Specifically, WSN jointly learns the model weights and task-adaptive binary masks that pertaining to subnetworks associated with each task. It also attemps to select a small set of weights to be activated *(winning ticket)* by reusing weights of the prior subnetworks.\n",
        "\n",
        "Similar to the CP&S, it updates only the weights that have not been trained on the previous tasks. After training for each task, the model freezes the subnetwork parameters. Therefore, WSN is also immune to the catastrophic forgetting by design just like CP&S. Its pruning (masking) approach a bit different though.  \n",
        "\n",
        "For example, in the figure, $task$ $t-1$ is learned by a orange subnetwork. In the $task$ $t$, we can still use the orange weights while doing a forward pass yet we cannot use them on the backward pass. It is only allowed for unassigned weights.\n",
        "\n",
        "* **Masking Method**: WSN tries to find best subnetwork by selecting the $c$% weights with the highest weight scores $s$, where $c$ is the target layerwise capacity ratio in %.\n",
        "\n",
        "* **Mask Selection**: ?\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/winningsubnetworks.png?raw=true\" width=700>\n",
        "</p>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CyCUBJLNCCXr"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN5nyFwaFTtrRpvva8tCU7b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science (SET)  \n",
        "##*Nature Communications 2018*\n",
        "\n",
        "Randomly initialize SCLs in our network and start training. \n",
        "\n",
        "At the end of each epoch, we remove the connections with the smallest weights (the “weakest” connections) based on a threshold $t$ and replace them with randomly initialized new ones. \n",
        "\n",
        "Repeat.\n",
        "\n",
        "SET turns out to be surprisingly robust and stable. Encouragingly, the authors are able to show very similar results to FCL models (sometimes surpassing their performance) with SET models that contain far fewer parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lCf1YYM9eev7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization (Dynamic Sparse Reparameterization - DSR)\n",
        "##*ICML 2019*\n",
        "\n",
        "Randomly initialize SCLs in our network and start training.\n",
        "\n",
        "Calculate mean magnitude of momentum $G$ for each layer .\n",
        "\n",
        "Remove connections with smallest weights based on global adaptive threshold $t$.\n",
        "\n",
        "Immediately after removing $K$ number of parameters during the pruning phase, $K$ zero-initialized parameters are redistributed among the sparse parameter tensors, based on calculated mean magnitude of gradient $G$: layers having larger fractions of non-zero weights receive proportionally more free parameters. This means, free parameters should be redistributed to layers whose parameters receive larger loss gradients.\n",
        "\n",
        "Repeat.\n"
      ],
      "metadata": {
        "id": "VW1HUmK7eezq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Networks from Scratch: Faster Training without Losing Performance (Sparse Momentum - SM)\n",
        "##*ICLR 2020*\n",
        "\n",
        "Randomly initialize SCLs in our network and start training.\n",
        "\n",
        "Calculate mean magnitude of momentum $M$ for each layer .\n",
        "\n",
        "Remove the smallest 50% of weights for each layer.\n",
        "\n",
        "Immediately after removing $K$ number of parameters during the pruning phase, $K$ zero-initialized parameters are redistributed among the sparse parameter tensors, based on calculated mean magnitude of momentum $M$: layers having larger fractions of mean momentum will receive proportionally more free parameters.\n",
        "\n",
        "Repeat.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/SM.jpg?raw=true\" width=700>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "gxboUvv9ee6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers (DST)\n",
        "##*ICLR 2020*\n",
        "\n",
        "Randomly initialize Fully Connected Neural Network.\n",
        "\n",
        "Randomly initialize trainable mask layers (layer-level threshold $t$) and start training on Masked Neural Network.\n",
        "\n",
        "Instead of pruning (masking) between two training epochs with a predefined pruning schedule, this method prunes and recovers the network parameters at each training step, which is far more fine-grained than existing methods.\n",
        "\n",
        "In each training step, parameter is subtracted with respective threshold value and it is going to be masked if the values is smaller than 0. Not masked or pruned, otherwise:\n",
        "\n",
        "$Q$<sub>ij</sub>= $F$($W$<sub>ij</sub> ,$t$<sub>i</sub>) = $|W$<sub>ij</sub>$|$ - $t$<sub>i</sub>\n",
        "\n",
        "$M$<sub>ij</sub> = $S(Q$<sub>ij</sub>$)$ where $M$<sub>ij</sub>$= 1$ if not pruned, $M$<sub>ij</sub> $= 0$ if pruned\n",
        "\n",
        "Repeat.\n",
        "\n",
        "However, authors realize that $t$<sub>i</sub> cannot be learnt or updated under this funtions $S(x)$ since its gradient is always equal to 0. Therefore, they come up with  approximation funtion $H(x)$ which allows to learn ti and consequenly all the mask layers since it has a gradient.\n",
        "\n",
        "Finally, after training, the model would be sparse based on trained mask layers (layer-level threshold $t$<sub>i</sub>).\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/DST.jpg?raw=true\" width=550>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ORhOuA9See8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\n",
        "##*CVPR 2018*\n",
        "#### Arun Mallya and Svetlana Lazebnik\n",
        "\n",
        "Inspired by network pruning techniques, PackNet exploits redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, PackNet is able to sequentially “pack” multiple tasks into a single network while ensuring minimal drop in performance and minimal storage overhead. To do that, after finding $n$<sup>th</sup> pack, it freezes and removes that \"pack\" from the large backbone. Hence, $n+1$<sup>th</sup> pack that will be constructed for the next task will not interfere the $n$<sup>th</sup> pack and it repeats same steps for the rest of the tasks to avoid catastrophic forgetting. In the figure, white circles represents available neurons in the backbone while bold circles indicates neurons that are already occupied in another pack that is why in the next pack selection these neurons will be discarded.\n",
        "\n",
        "* **Masking Method**: Train the all backbone then remove 50% or 75% of the connections based on weights' absolute magnitude. Re-train.\n",
        "* **Mask Selection**: -\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/packnet.png?raw=true\" width=800>\n",
        "</p>"
      ],
      "metadata": {
        "id": "jM5KlF8g1zLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights\n",
        "## *ECCV 2018*\n",
        "#### Arun Mallya, Dillon Davis, and Svetlana Lazebnik\n",
        "\n",
        "Inspired by PackNET which adopts a different route by iteratively pruning unimportant weights and fine-tuning them for learning new tasks. \n",
        "Piggyback questions whether the weights of a network have to be changed at all. It suggest we might get reasonable results with just selectively masking, or setting certain weights to 0, while keeping the rest of the weights the same as before. \n",
        "\n",
        "Based on this idea, Piggyback learns how to mask weights of an existing “backbone” network for obtaining good performance on a new task, as shown in the Figure. Binary masks that take values in {0, 1} are learned and stored after each task. This simple idea is mostly suitable for task-IL scenario.\n",
        "\n",
        "* **Masking Method**:\n",
        "* **Mask Selection**: -\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/piggyback.png?raw=true\" width=700>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "DK7rElqguenW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SupSup: Supermasks in Superposition\n",
        "## *NeurIPS 2020*\n",
        "#### Mitchell Wortsman et al.\n",
        "\n",
        "Supermasks in Superposition (SupSup) model uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask). If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. Authors experimentally find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. Hence, SupSup is suitable for class-IL as well.\n",
        "\n",
        "\n",
        "During training, SupSup learns a separate supermask (subnetwork) for each task. At inference time, SupSup can infer task identity by superimposing all supermasks. Ideally, appropriate supermask for a given task should exhibit a confident output distribution (i.e. low entropy).\n",
        "\n",
        "* **Masking Method**: Deconstructing lottery tickets: Zeros, signs, and the supermask.\n",
        "* **Mask Selection**: Try all the supermasks, return mask with a lowest entropy\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/supsup.png?raw=true\" width=800>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ItpNEe7__RUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpaceNet: Make Free Space for Continual Learning\n",
        "## *Neurocomputing 2021*\n",
        "#### Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy\n",
        "\n",
        "SpaceNet trains sparse deep neural networks from scratch to have compact number of neurons for each task. The adaptive training of the sparse connections results in sparse representations per task that reduce the interference time. Experimental results show the robustness of the method against catastrophic forgetting and leaving space for more tasks to be learned. \n",
        "\n",
        "When the model faces a new task, new sparse connections are randomly allocated between a selected number of neurons in each layer. At the end of the training, the initial distribution of the connections changes and connections that are important for that task group together.\n",
        "The most important neurons for a specific task are reserved to be specific to this task, and will not be seen by the following tasks and will freeze. However, other neurons that are somehow important or not important at all will continue to be shared between the tasks. For example, in the figure, fully filled circles represent the neurons that are most important and become specific for $task$ $t$, where partially filled ones are less important and could be shared by other tasks. Multiple colored circles represent the neurons that are used by multiple tasks. After learning $task$ $t$, the corresponding weights are kept fixed.\n",
        "\n",
        "For convolutional neural networks, SpaceNet performs a coarse manner in drop and grow phases to impose structure sparsity instead of irregular sparsity. In particular, in the drop phase, coarse removal for the whole kernel is applied instead of removing scalar weights. Similarly, in the grow phase, the whole connections of a kernel are added instead of adding single weights. The likelihood of adding a kernel between two feature maps is inversely proportional to their significance, similar to multilayer perceptron networks. The feature map's importance is determined by adding the importance of its connected kernels.\n",
        "\n",
        "* **Masking Method**: A fraction $r$ of the sparse connections in each layer is dynamically changed based on the importance of the connections and neurons in that layer. Connection importance is estimated by its contribution to the change in the loss function. The first-order Taylor approximation is used to approximate the change in loss during one training iteration $i$. Growing and Dropping the connection is applied based on the importance score.\n",
        "\n",
        "* **Mask Selection**: Use the whole network structure, no masking.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/spacenet.png?raw=true\" width=700>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "0W2HuqQfIoJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Prune-and-Select (CP&S): Class-Incremental Learning with specialized subnetworks\n",
        "##*2022*\n",
        "#### Aleksandr Dekhovich, David M.J. Tax, Marcel H.F. Sluiter, and Miguel A. Bessa\n",
        "\n",
        "During training, Continual-Prune-and-Select (CP&S) finds a subnetwork within the DNN that is responsible for solving a given $task$ $t$. \n",
        "\n",
        "A new task is learned by training available neuronal connections (previously untrained) of the DNN to create a new subnetwork which can include previously trained connections belonging to other subnetwork(s) but those will not be updated. In other words, previously trained connections can be shared between tasks yet cannot be updated.\n",
        "\n",
        "Then, during inference, CP&S selects the correct subnetwork to make predictions for that task.  This enables to eliminate catastrophic forgetting by creating specialized regions in the DNN that do not conflict with each other while still allowing knowledge transfer across them. \n",
        "\n",
        "\n",
        "* **Masking Method**: NNrelief\n",
        "* **Mask Selection**: Try all the subnetworks, return mask with a lowest entropy (*maximum output response*)"
      ],
      "metadata": {
        "id": "y40AC2v9facf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forget-free Continual Learning with Winning Subnetworks\n",
        "##*ICML 2022*\n",
        "#### Haeyong Kang et al.\n",
        "\n",
        "For each task, the WSN sequentially learns and chooses the best subnetwork. Specifically, WSN jointly learns the model weights and task-adaptive binary masks that pertaining to subnetworks associated with each task. It also attemps to select a small set of weights to be activated *(winning ticket)* by reusing weights of the prior subnetworks.\n",
        "\n",
        "Similar to the CP&S, it updates only the weights that have not been trained on the previous tasks. After training for each task, the model freezes the subnetwork parameters. Therefore, WSN is also immune to the catastrophic forgetting by design just like CP&S. Its pruning (masking) approach a bit different though.  \n",
        "\n",
        "For example, in the figure, $task$ $t-1$ is learned by a orange subnetwork. In the $task$ $t$, we can still use the orange weights while doing a forward pass yet we cannot use them on the backward pass. It is only allowed for unassigned weights.\n",
        "\n",
        "* **Masking Method**: WSN tries to find best subnetwork by selecting the $c$% weights with the highest weight scores $s$, where $c$ is the target layerwise capacity ratio in %.\n",
        "\n",
        "* **Mask Selection**: ?\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/winningsubnetworks.png?raw=true\" width=700>\n",
        "</p>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CyCUBJLNCCXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Squeeze-and-Excitation Networks\n",
        "##*CVPR 2018*\n",
        "#### Jie Hu, Li Shen and Gang Sun\n",
        "\n",
        "“Squeeze-and- Excitation” (SE) blocks try to improve the representational power of a network by explicitly modelling the interdependencies between the channels of its convolutional features. To achieve this, it uses the global information to selectively emphasise informative features while suppressing less useful ones.\n",
        "\n",
        "The basic structure of the SE block is illustrated in figure. For any given transformation (e.g. a convolution or a set of convolutions), in features are first passed through a *squeeze* operation. It aggregates the feature maps across spatial dimensions to produce a channel descriptor with global pooling (CxHxW -> Cx1x1). \n",
        "\n",
        "This is followed by an *excitation* operation, in which channel-specific activations obtained by sigmoid function are learned for each channel. In features are then reweighted by corespondent channel-specific activations. At the end, it helps to consider important channels (or feature maps) heavily than the others for a given input set with a such simple attention mechanism.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/squeezeandexcitation.png?raw=true\" width=800>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "Ar7TVA653giG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional networks with adaptive inference graphs\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "hV7YeaZK2UsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Powerpropagation: A sparsity inducing weight reparameterisation\n",
        "## *NeurIPS 2021*\n",
        "#### Jonathan Schwarz et al.\n",
        "\n",
        "A weight-parameterization method for neural networks called powerpropagation produces models that are inherently sparse. Exploiting the behaviour of gradient descent, Powerprop gives rise to weight updates exhibiting a “rich get richer” dynamic by leaving low-magnitude parameters largely unaffected by learning. As a result, models trained in this way perform similarly but have a distribution with a noticeably higher density at zero, enabling the safe pruning of more parameters.\n",
        "\n",
        "In the forward pass of a neural networks, raise the parameters of the model to the α-th power (where α > 1) while preserving the sign. Parameters that are raised to α − 1 will appear in the gradient computation, scaling the usual update. Because of this, larger magnitude parameters receive larger gradient updates than smaller magnitude parameters do, resulting in the previously mentioned \"rich get richer\" phenomenon.\n",
        "\n",
        "In a simple formulation where w = v|v|<sup>α−1</sup>, for any arbitrary power α ≥ 1 we preserved the sign of v so that it can still represent both negative and positive values. For α = 1 this recovers the standard backpropagation setting. For α ≥ 1, updates in the weights are naturally obtained by the standard backpropagation but with enhanced gradients to enforce \"rich get richer\" phenomenon.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/powerprop.png?raw=true\" width=900>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "dBz0OgUfCO-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning without Forgetting\n",
        "## *TPAMI 2017*\n",
        "#### Zhizhong Li and Derek Hoiem\n",
        "\n",
        "Learning without Forgetting (LwF) approach could be seen as a combination of Distillation Networks and fine-tuning. Fine-tuning modifies the parameters of an existing CNN to train a new task. A small learning rate is often used, and sometimes part of the network is frozen to prevent overfitting. Distillation Networks helps simpler networks to return more reasonable outputs by providing additional info from the input data.\n",
        "\n",
        "In LwF, let number of tasks is equal to $t$ and let tasks are defined in a class-incremental manner. Then each previous task's network will become a distillation network of the following task. (e.g. network of the $task$ $t-1$ will be assigned as a distillation network of $task$ $t$.) This distillation network will guide the main network of the current task while fine tuning so that it will not forget the previously learned tasks:\n",
        "\n",
        "0. Add $c$ number of nodes to the classifier of old network (distillation network) from $task$ $t-1$ to create new network for $task$ $t$.\n",
        "1. Forward input of the $task$ $t$ to both networks.\n",
        "2. Name logits of the distillation as *soft targets* and classes of the input of $task$ $t$ as *hard targets*.\n",
        "3. Compare hard targets with the predicted labels to calculate $cross$-$entropy$ $loss$.\n",
        "4. Compare soft targets with logits of the new network to calculate $distillation$ $loss$. (ignore newly added $c$  number of node(s) - in other words, consider only old classes which should not be forget)\n",
        "5. Initiate backbropagation with **Loss** so that new network will be enforced to learn new task while preseving the old taks by distilled information where\n",
        "\n",
        "    **Loss** = $cross$-$entropy$ $loss$ + λ*$distillation$ $loss$.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/lwf.png?raw=true\" width=500>\n",
        "</p>"
      ],
      "metadata": {
        "id": "QY9S1-Vb_iQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overcoming catastrophic forgetting in neural networks\n",
        "## *PNAS 2017*\n",
        "#### James Kirkpatrick et al.\n",
        "\n",
        "This paper developed an algorithm analogous to synaptic consolidation for artificial neural networks, which is referred as elastic weight consolidation (EWC). This algorithm slows down learning (or changing) on certain weights based on how important they are to previously seen tasks. This importance is called Fisher information matrix $F$ and it has three key properties: (i) It is equivalent to the second derivative of the loss near a minimum, (ii) it can be computed from first-order derivatives alone and is thus easy to calculate even for large models, and (iii) it is guaranteed to be positive semidefinite.\n",
        "\n",
        "Overall, the loss function that we try to minimize in EWC is:\n",
        "\n",
        "$L$(θ) = $L$<sub>B</sub>(θ) + ∑ λ / 2 * $F$<sub>i</sub>(θ<sub>i</sub> + θ<sub>A,i</sub>)<sup>2</sup> \n",
        "\n",
        "where\n",
        "\n",
        "$L$<sub>B</sub>(θ): is the loss for latter task B only,\n",
        "\n",
        "λ: sets how important the old task A is compared with the new one and,\n",
        "\n",
        "i: labels each parameter.\n",
        "\n",
        "EWC will attempt to keep the network parameters close to the learned parameters of both tasks A and B when switching to a third task. This can be enforced either with two separate penalties or as one by noting that the sum of two quadratic penal- ties is itself a quadratic penalty.\n",
        "\n",
        "In the figure, after learning the first task, the parameters θ<sub>A</sub><sup>*</sup> were obtained. If we take gradient steps according to task B alone (blue arrow), we will minimize the loss of task B but destroy what we have learned for task A. However, if we impose an excessive amount of limitation by assigning the same coefficient to each weight (green arrow), we can only recall task A at the risk of failing to learn task B. EWC, on the other hand, clearly calculates how important weights are for job A in order to discover a solution for task B without suffering a major loss on task A (red arrow).\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/ewc.png?raw=true\" width=450>\n",
        "</p>"
      ],
      "metadata": {
        "id": "ylX87elZS-CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Learning Through Synaptic Intelligence\n",
        "## *ICML 2017*\n",
        "#### Friedemann Zenke, Ben Poole and Surya Ganguli\n",
        "\n",
        "...\n"
      ],
      "metadata": {
        "id": "dCQi7IyNz_hL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradmax: Growing neural networks using gradient information\n",
        "\n",
        "Gradmax aims to grow the network architecture without costly retraining. It adds new neurons during training without impacting what is already learned, while improving the training dynamics by maximizing the gradients of the new weights and efficiently find the optimal initialization by means of the singular value decomposition (SVD). \n",
        "\n",
        "It starts with a small seed architecture. Then over the course of the training, new neurons are added to the seed architecture: either increasing the width of the existing layers or creating new layers. \n",
        "\n",
        "In the illustration of Gradmax, growing neurons require initializing incoming W<sub>l</sub><sup>new</sup> and outgoing W<sub>l+1</sub><sup>new</sup> weights for the new neuron. GradMax sets incoming weights W<sub>l</sub><sup>new</sup> to zero (dashed lines) in order to keep the output unchanged so that backprop will not impact the currently learned weights. It initializes outgoing weights W<sub>l+1</sub><sup>new</sup> using SVD. This maximizes the gradients on the incoming weights with the aim of accelerating training.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/gradmax.png?raw=true\" width=700>\n",
        "</p>"
      ],
      "metadata": {
        "id": "JZHwHfKY2dAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning\n",
        "##*ICML 2022*\n",
        "#### Utku Evci, Vincent Dumoulin, Hugo Larochelle and Michael C. Mozer\n",
        "\n",
        "Head-to-Toe probing (Head2Toe) selects features from all layers of the source model to train a classification head for the target domain. It aims to replace traditional transfer learning approach by assuming relevant feature maps can occur anywhere in the network instead of the last layer.\n",
        "\n",
        "It connects outputs (feature maps) of the all layers with classifier head. And then applies Lasso Regulizer to select only relevant features for the classifier. Since Lasso force connections to be zero if they are irrevelant only important featur maps would contribute to the output.\n",
        "\n",
        "Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/head2toe.png?raw=true\" width=350>\n",
        "</p>"
      ],
      "metadata": {
        "id": "GvZdRgS89dpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win\n",
        "## *AAAI 2022*\n",
        "#### Utku Evci, Yani Ioannou, Cem Keskin and Yann Dauphin\n",
        "\n",
        "This paper investigates why training unstructured sparse networks from random initialization performs poorly and what makes Lottey Tickets (LTs) and Dynamic Sparse Training (DST) exceptions? \n",
        "\n",
        "And it is found that Sparse NNs have poor gradient flow at initialization. Hence, the importance of using sparsity-aware initialization is demonstrated. Furthermore, DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, the success of LTs lies in re-learning the pruning solution from which they are derived, not in improving gradient flow.\n",
        "\n"
      ],
      "metadata": {
        "id": "QzrXu6ZY9opE"
      }
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYpKNWRg6Fmjui8fIw25It"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science (SET)  \n",
        "## Nature Communications 2018\n",
        "\n",
        "#### Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H. Nguyen, Madeleine Gibescu & Antonio Liotta \n",
        "\n",
        "Randomly initialize SCLs in our network and start training. \n",
        "\n",
        "At the end of each epoch, we remove the connections with the smallest weights (the “weakest” connections) based on a threshold $t$ and replace them with randomly initialized new ones. Repeat.\n",
        "\n",
        "SET turns out to be surprisingly robust and stable. Encouragingly, the authors are able to show very similar results to FCL models (sometimes surpassing their performance) with SET models that contain far fewer parameters.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lCf1YYM9eev7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overcoming Catastrophic Forgetting with Hard Attention to the Task (HAT)\n",
        "## PMLR 2018\n",
        "#### Joan Serra, Didac Suris, Marius Miron, Alexandros Karatzoglou\n",
        "\n",
        "This paper argues that masks should be learnable, instead of heuristically or rule driven pre-defined unlike pruning and sparse training. Therefore, HAT does not assign pre-defined compression ratios nor determine parameter importance through a post-training step. It is learned during training with an attention mechanism. A task-based hard attention mechanism maintains the information from previous tasks without affecting the learning of a new task. This mechanism is placed after each layer so that task embeddings are updated and learned during training as well. To binarize this attention mechanism, HAT utilizes a positive scaler factor (s) the sigmoid function (σ) to decide whether the layer should be activated for a given task. To sum up, HAT gives the neural network models an ability to learn which layers should activated for a task $t$ without any heuristically pre-defined rule.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/hat.png?raw=true\" width=450>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "E1I2qWb--EG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n",
        "## ICLR 2019\n",
        "#### Jonathan Frankle, Michael Carbin\n",
        "\n",
        "Most of the experiences showed that sparse architectures produced by pruning are difficult to train from the start. This paper, however, present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. It suggests that: 'A randomly-initialized, dense neural network contains a subnetwork that is initialized such that when trained in isolation, it can match the test accuracy of the original network after training for at most the same number of iterations.' \n",
        "\n",
        "In the experiments, they identified a winning ticket by training a network and pruning smallest-magnitude weights. The remaining, unpruned connections, constitute the architecture of the winning ticket. After that, each unpruned connection’s value was reset to its initialization from the original network before it was trained. Finally, pruned but re-initialized sparse network trained with same computational budget. It turned out the performance scores of the sparse neural network were similar with original dense network.\n",
        "\n",
        "It was also suggested that using iterative pruning alleviate to find winning tickets that match the accuracy of the original network at smaller sizes than does one-shot pruning. To prove that, authors set a *n round* rule and at the end of each round they pruned %p of the network, re-initialized it and continue the learning process. \n",
        "\n",
        "This was one of the first papers that paved the way for dynamic sparse training literature. "
      ],
      "metadata": {
        "id": "33DpAFebWbKE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization (Dynamic Sparse Reparameterization - DSR)\n",
        "##ICML 2019\n",
        "#### Hesham Mostafa, Xin Wang\n",
        "\n",
        "Randomly initialize SCLs in our network and start training.\n",
        "\n",
        "Calculate mean magnitude of gradient $G$ for each layer .\n",
        "\n",
        "Remove connections with smallest weights based on global adaptive threshold $t$.\n",
        "\n",
        "Immediately after removing $K$ number of parameters during the pruning phase, $K$ zero-initialized parameters are redistributed among the sparse parameter tensors, based on calculated mean magnitude of gradient $G$: layers having larger fractions of non-zero weights receive proportionally more free parameters. This means, free parameters should be redistributed to layers whose parameters receive larger loss gradients. Repeat.\n"
      ],
      "metadata": {
        "id": "VW1HUmK7eezq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rigging the Lottery: Making All Tickets Winners\n",
        "## ICML 2019\n",
        "#### Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro and Erich Elsen\n",
        "\n",
        "According to the Lottery Ticket Hypothesis, if we can identify a sparse neural network with iterative pruning, then we can train that sparse network from scratch to the same degree of accuracy by beginning from the initial conditions.\n",
        "\n",
        "Motivating from that, Rigging the Lottery or *RigL* updates the topology of the sparse network during training based on parameter magnitudes and infrequent gradient calculations.\n",
        "\n",
        "RigL starts with a random sparse network, and at regularly spaced intervals it removes a fraction of connections based on their magnitudes and activates new ones using instantaneous gradient information. It grows the connections with highest magnitude gradients which brings novelty to this method. Newly activated connections are initialized to zero and therefore don’t affect the output of the network. However they are expected to receive gradients with high magnitudes in the next iteration and therefore reduce the loss fastest.\n",
        "\n",
        "RigL was able to find more accurate models than the current best dense-to-sparse training algorithms.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/rigl.png?raw=true\" width=450>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "lIXrgCIL99Vv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SNIP: Single-Shot Network Pruning Based On Connection Sensitivity\n",
        "## ICLR 2019\n",
        "#### Namhoon Lee, Thalaiyasingam Ajanthan, Philip H. S. Torr\n",
        "\n",
        "In this work, authors present an approach that prunes a given network once at initialization prior to training. To achieve this, a saliency criterion based on connection sensitivity was introduced. Saliency criterion identifies structurally important connections in the network for the given task. This eliminates the need for both pretraining and the complex pruning schedule while making it robust to architecture variations. Here, weight initialization method has a significance since saliency scores are calculated directly from the initial weights without training. Therefore, authors advocate the use of variance scaling methods so that variance remains the same throughout the network.\n",
        "\n",
        "While computing the saliency scores, one batch from the dataset is sampled. Then, mask values ($m$) which is initially equal to 1 of each weight is modified a bit ($m-δ$ or $1-δ$) and change in the loss is observed sequentially.  Finally, weights with smallest saliency scores are removed based on the predefined sparsity level. After pruning, the sparse network is trained in the standard way."
      ],
      "metadata": {
        "id": "3w8IFuS5oqf8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Networks from Scratch: Faster Training without Losing Performance (Sparse Momentum - SM)\n",
        "##ICLR 2020\n",
        "#### Tim Dettmers, Luke Zettlemoyer\n",
        "\n",
        "Randomly initialize SCLs in our network and start training.\n",
        "\n",
        "Calculate mean magnitude of momentum $M$ for each layer .\n",
        "\n",
        "Remove the smallest 50% of weights for each layer.\n",
        "\n",
        "Immediately after removing $K$ number of parameters during the pruning phase, $K$ zero-initialized parameters are redistributed among the sparse parameter tensors, based on calculated mean magnitude of momentum $M$: layers having larger fractions of mean momentum will receive proportionally more free parameters. Repeat.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/SM.jpg?raw=true\" width=700>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "gxboUvv9ee6F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Top-KAST: Top-K Always Sparse Training\n",
        "## NeurIPS 2020\n",
        "#### Siddhant M. Jayakumar, Razvan Pascanu, Jack W. Rae, Simon Osindero and Erich Elsen\n",
        "\n",
        "In this paper, authors aim to propose a fully sparse training approach called Top-KAST. It is suggested that Top-KAST is scalable because it never requires a forward pass with dense parameters, nor calculating a dense gradient.\n",
        "\n",
        "To be able to that, it selects a subset (*A*) of dense network(θ) with a sparsity level of **1-D** where *D* is the desired density. Subset *A* is used in forward-pass and predictions. While in the backprop, another subset (*B*) of dense network (θ) is employed where subset *B* covers subset *A*. Hence, instead of calculating gradients of the whole dense network, it only calculates a subset of it to grow new connections after removing connections with the lowest magnitudes.\n",
        "\n",
        "In other words, Top-KAST is a RigL with a sparse backpropagation."
      ],
      "metadata": {
        "id": "5Y38cnzm_9B5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers (DST)\n",
        "##ICLR 2020\n",
        "\n",
        "Randomly initialize Fully Connected Neural Network.\n",
        "\n",
        "Randomly initialize trainable mask layers (layer-level threshold $t$) and start training on Masked Neural Network.\n",
        "\n",
        "Instead of pruning (masking) between two training epochs with a predefined pruning schedule, this method prunes and recovers the network parameters at each training step, which is far more fine-grained than existing methods.\n",
        "\n",
        "In each training step, parameter is subtracted with respective threshold value and it is going to be masked if the values is smaller than 0. Not masked or pruned, otherwise:\n",
        "\n",
        "$Q$<sub>ij</sub>= $F$($W$<sub>ij</sub> ,$t$<sub>i</sub>) = $|W$<sub>ij</sub>$|$ - $t$<sub>i</sub>\n",
        "\n",
        "$M$<sub>ij</sub> = $S(Q$<sub>ij</sub>$)$ where $M$<sub>ij</sub>$= 1$ if not pruned, $M$<sub>ij</sub> $= 0$ if pruned\n",
        "\n",
        "Repeat.\n",
        "\n",
        "However, authors realize that $t$<sub>i</sub> cannot be learnt or updated under this funtions $S(x)$ since its gradient is always equal to 0. Therefore, they come up with  approximation funtion $H(x)$ which allows to learn ti and consequenly all the mask layers since it has a gradient.\n",
        "\n",
        "Finally, after training, the model would be sparse based on trained mask layers (layer-level threshold $t$<sub>i</sub>).\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/DST.jpg?raw=true\" width=550>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ORhOuA9See8A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topological Insights into Sparse Neural Networks\n",
        "## ECML 2020\n",
        "#### Shiwei Liu, Tim Van der Lee, Anil Yaman, Zahra Atashgahi, Davide Ferraro, Ghada Sokar, Mykola Pechenizkiy, and Decebal Constantin Mocanu\n",
        "\n",
        "In this work, authors proposed Neural Network Sparse Topology Distance (NNSTD) to measure the distance between different sparse neural networks. As a result, they showed that adaptive sparse connectivity can always unveil a plenitude of sparse subnetworks with very different topologies which outperform the dense model. This finding complements the Lottery Ticket Hypothesis since it is showing that there is a much more efficient and robust way to find “winning tickets”. As a result, randomly initialized sparse neural networks with adaptive sparse connectivity offer benefits not just in terms of computational and memory costs, but also in terms of the principal performance criteria for neural networks, e.g. accuracy for classification tasks.\n",
        "\n",
        "Hence, in the light of the foundings, instead of exploring all resources to train over-parameterized models, intrinsically sparse networks with topological optimizers were suggested."
      ],
      "metadata": {
        "id": "bI8KcESGMXWG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Picking Winning Tickets Before Training by Preserving Gradient Flow \n",
        "## ICLR 2020\n",
        "#### Chaoqi Wang, Guodong Zhang, Roger Grosse \n",
        "\n",
        "Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. Therefore, authors aimed to apply pruning at network initialization which saves resources at training time as well. They emphasized that efficient training requires preserving the gradient flow through the network. Finally, they proposed a simple but effective pruning criterion called Gradient Signal Preservation (GraSP).\n",
        "\n",
        "The idea of the GraSP is highly close to SNIP which aims to preserve the loss of the original randomly initialized network. GraSP, however, argues that the loss is no better than chance at initialization. Hence, at beginning of training, it is more important to preserve the training dynamics than the loss itself. SNIP does not carry out this action automatically because, even if cutting off a specific connection has no impact on the loss, it might still obstruct the flow of information across the network. For instance, authors noticed that SNIP with a high pruning ratio (e.g. 99%) tends to eliminate nearly all the weights in a particular layer which creates a bottleneck in the network. Therefore, they went to different direction which considers how the presence or absence of one connection influences the training of the rest of the network.\n",
        "\n",
        "Mathematically, a larger gradient norm indicates that each gradient update achieves a greater loss reduction. Therefore, authors aimed to preserve or even increase (if possible) the gradient flow after pruning. They cast the pruning operation as adding a perturbation δ to the initial weights and use a Taylor approximation to characterize how removing one weight will affect the gradient flow after pruning.\n",
        "\n",
        "GraSP compute the score of each weight, which reflects the change in gradient flow after pruning the weight. Specifically, if the weight's score is negative, then removing the corresponding weights will reduce the gradient flow, while if it is positive, it will increase the gradient flow. Therefore,the larger the score of a weight the lower its importance. For a given pruning ratio p, sparse model is found by removing the top p fraction of the weights.\n",
        "\n"
      ],
      "metadata": {
        "id": "9zdUpLd1qrTs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pruning Neural Networks Without Any Data by Iteratively Conserving Synaptic Flow\n",
        "## NeurIPS 2020\n",
        "#### Hidenori Tanaka, Daniel Kunin, Daniel L. K. Yamins, Surya Ganguli\n",
        "\n",
        "Video Explanation: https://www.youtube.com/watch?v=8l-TDqpoUQs\n",
        "\n",
        "This study tries to identify highly sparse trainable subnetworks at initialization, without ever training, or without ever looking at the data While trying to reach this goal, they reached the following conclusions:\n",
        "\n",
        "i. The premature pruning of an entire layer making a network untrainable due to layer-collapse. Hence, Maximal Critical Compression is formulated that posits a pruning algorithm should avoid layer-collapse whenever possible. It basically suggests that pruning algorithm should never prune a set of parameters that results in layer-collapse if there exists another set of the same cardinality that will keep the network trainable.\n",
        "    \n",
        "Max compression (ρ<sub>max</sub> = Number of Parameters / Number of Layers) is the maximal possible compression ratio for a network that doesn’t lead to layer-collapse. Hence, compression ratio (ρ<sub>cr</sub>) should be smaller than equal to  max compression: ρ<sub>cr</sub> ≤ ρ<sub>max</sub>\n",
        "\n",
        "ii. It is demonstrated that *synaptic saliency*, a general class of gradient-based scores for pruning, is conserved at every hidden unit and layer of a neural network according to 'Conservation Laws of Synaptic Salience'. \n",
        "\n",
        "Theory 1 Neuron-wise Conservation of Synaptic Saliency: For a feedforward neural network, the sum of the synaptic saliency for the incoming parameters to a hidden neuron is equal to the sum of the synaptic saliency for the outgoing parameters from the hidden neuron.\n",
        "\n",
        "Theory 2 Network-wise Conservation of Synaptic Saliency: The sum of the synaptic saliency is same across any set of parameters that exactly separates the input neurons x from the output neurons y of a feedforward neural network.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/synflow.png?raw=true\" width=800>\n",
        "</p>\n",
        "\n",
        "Consider the set of parameters in a layer of a neural network. This set would exactly separate the input neurons from the output neurons. Thus, by the network-wise conservation of synaptic saliency (law 2), the total score for this set is constant for all layers, implying the average is inversely proportional to the layer size. Therefore, parameters per layer in large layers receive lower scores than parameters in small layers. That is why single-shot pruning disproportionately prunes the largest layer leading to layer-collapse.\n",
        "\n",
        "However, if conservation laws are coupled with iterative magnitude pruning (IMP), layer-collapse is avoided. Because, when the largest layer is pruned, becoming smaller, then in subsequent iterations the remaining parameters of this layer will receive higher relative scores. So, two key ingradients can be identified for IMP’s ability to avoid layer-collapse: (i) approximate layer-wise conservation of the pruning scores, and (ii) the iterative re-evaluation of these scores.\n",
        "\n",
        "iv. Data-agnostic algorithm that satisfies Maximal Critical Compression is introduced and referred as Iterative Synaptic Flow Pruning (SynFlow). This algorithm can be interpreted as preserving the total flow of synaptic strengths through the network at initialization subject to a sparsity constraint. Thus this data-agnostic pruning algorithm challenges the existing paradigm that, at initialization, data must be used to quantify which synapses are important.\n",
        "\n",
        "Algorithm works as follows:\n",
        "\n",
        "First, all parameters are converted to their absolute values. Second, a data point that is filled with 1's(ones) e.g. an image with a pixel values of 1 simply fed through the network with all of these positive weights and an output vector is obtained. Then each element in the output vector is sum up and get a single number which is R, pseudo loss function value. \n",
        "\n",
        "And then, loss is back propagated to the layers so, score is going to be the derivative of R with respect to weight multiplied by that weight. In this way, score for each parameter would be calculated which is easy and not expensive. Finally, parameters that are smaller than the threshold value will be pruned.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/synflow1.png?raw=true\" width=650>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "taGRUr_5_Ikv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embracing Change: Continual Learning in Deep Neural Networks\n",
        "## Trends in Cognitive Sciences 2020\n",
        "#### Raia Hadsell, Dushyant Rao, Andrei A. Rusu, Razvan Pascanu\n",
        "\n",
        "Paper shortly describes what continual learning is, why it is important and how closely related with human learning. Furthermore, it provides desiderata for continual learning and current approaches, regularization-based, architecture-based (also mentioned as modular/ity architectures), memory-based, and metalearning-based to satisfy those conditions. \n",
        "\n",
        "Regularization-based approaches directly modify the optimization of neural networks and have been shown to reduce catastrophic forgetting. Modular architectures offer pragmatic solutions to interference and catastrophic forgetting, while enabling forward transfer through hierarchical recomposition of skills and knowledge. End-to-end memory models could be a scalable solution for long timescale learning, and meta-learning approaches could surpass hand-designed algorithms and architectures altogether.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/desiderata_cl.png?raw=true\" width=800>\n",
        "</p>"
      ],
      "metadata": {
        "id": "l2doj-WltifM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse Evolutionary Deep Learning with Over One Million Artificial Neurons on Commodity Hardware\n",
        "## Neural Computing and Applications 2021\n",
        "#### Shiwei Liu, Decebal Constantin Mocanu, Amarsagar Reddy Ramapuram Matavalam, Yulong Pei & Mykola Pechenizkiy \n",
        "\n",
        "Off-the-shelf sparsity-inducing techniques either operate from a pretrained model or enforce the sparse structure via binary masks. Thus, the training efficiency is only demonstrated theoretically not practically. In this paper, authors introduce a technique allowing to train truly sparse neural networks im practice as well. They present a new way to represent sparse connections because most of the hardwares and softwares works with only dense representations.\n",
        "\n",
        "Experimental results demonstrate that the method can be applied directly to handle high-dimensional data, while achieving higher accuracy than the traditional two-phase(train and prune) approaches, it especially overcomes the 'curse of dimensionality'."
      ],
      "metadata": {
        "id": "Aphdi57DGL5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sparse Training via Boosting Pruning Plasticity with Neuroregeneration\n",
        "## NeurIPS 2021\n",
        "#### Shiwei Liu, Tianlong Chen, Xiaohan Chen, Zahra Atashgahi, Lu Yin, Huanyu Kou, Li Shen, Mykola Pechenizkiy, Zhangyang Wang, Decebal Constantin Mocanu\n",
        "\n",
        "Works on lottery ticket hypothesis (LTH) and single-shot network pruning (SNIP) have raised a lot of attention to post-training pruning (iterative magnitude pruning), and before-training pruning (pruning at initialization). The post-training pruning (iterative magnitude pruning) method suffers from an extremely large computation cost and the before-training pruning (pruning at initialization) usually struggles with insufficient performance. In comparison, during-training pruning, a class of pruning methods that simultaneously enjoys the training/inference efficiency and the comparable performance has been less explored. To measure the effect of pruning throughout training, study selects the performance metric as **pruning plasticity** (the ability of the pruned networks to recover the original performance). It was also found that the pruning plasticity can be substantially improved by injecting a brain-inspired mechanism called neuroregeneration, i.e., to regenerate the same number of connections as pruned. Designed novel gradual magnitude pruning (GMP) method advances state of the art, most impressively, its sparse-to-sparse version boosts the training performance over various dense-to-sparse methods.\n",
        "\n",
        "The study found that:\n",
        "\n",
        "i. Both pruning rate and learning rate matter for pruning plasticity. When pruned with low pruning rates, both dense-to-sparse training and sparse-to-sparse training can easily recover from pruning. On the contrary, if too many parameters are removed at one time, almost all models suffer from accuracy drops. Hence, gradually increasing the sparsity level boosts the performance of both dense-to-sparse training and sparse-to-sparse approach.\n",
        "\n",
        "ii. Neuroregeneration improves pruning plasticity. While regenerating the same number of connections as pruned, the pruning plasticity is observed to improve remarkably, indicating a more neuroplastic model being developed. However, it increases memory and computational overheads in dense-to-sparse training. That is why it is more suitable for sparse-to-sparse training: Gradual pruning starts with a sparse subnetwork and gradually prune the subnetwork to the target sparsity during training."
      ],
      "metadata": {
        "id": "UgusOyS1r2eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training \n",
        "##ICLR 2022\n",
        "#### Shiwei Liu , Tianlong Chen, Xiaohan Chen, Li Shen, Decebal Constantin Mocanu, Zhangyang Wang, Mykola Pechenizkiy\n",
        "\n",
        "Although random pruning is uncompetitive compared to post-training pruning or sparse training, it is arguably the most simplistic method of achieving sparsity in neural networks. In this research, authors emphasize a counter-intuitive result regarding sparse training: random pruning at initialization can be highly effective. In the study, it was empirically demonstrated that sparse training with randomly pruned network from beginning can match the performance of its dense equivalent, without the use of sophisticated pruning criteria or carefully considered sparsity structures. There are two key factors that contribute to this revival:\n",
        "\n",
        "i. the network size: It is discovered that the efficiency of sparse training with random pruning depends on the network size. Even with low sparsities (10%, 20%), random pruning hardly ever equals the full accuracy in tiny dense networks. However, the performance of randomly pruned sparse network will quickly catch up to its dense version as the networks get bigger and deeper even at high sparsity ratios.\n",
        "\n",
        "ii. appropriate layer-wise sparsity ratio: It is also found that, especially for large networks, selecting the right layer-wise sparsity initialization can be a significant help in training a randomly pruned network from scratch. The performance of a completely random sparse Wide ResNet-50 which is ERK initialized can outperform a densely trained Wide ResNet-50 on ImageNet."
      ],
      "metadata": {
        "id": "NBTHNMKfr8H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win\n",
        "## AAAI 2022\n",
        "#### Utku Evci, Yani Ioannou, Cem Keskin and Yann Dauphin\n",
        "\n",
        "This paper investigates why training unstructured sparse networks from random initialization performs poorly and what makes Lottey Tickets (LTs) and Dynamic Sparse Training (DST) exceptions.\n",
        "\n",
        "And it is found that Sparse NNs have poor gradient flow at initialization. Hence, the importance of using sparsity-aware initialization is demonstrated. Furthermore, DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, the success of LTs lies in re-learning the pruning solution from which they are derived, not in improving gradient flow.\n",
        "\n"
      ],
      "metadata": {
        "id": "QzrXu6ZY9opE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparsity May Cry"
      ],
      "metadata": {
        "id": "fvNofpZjsCZ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PackNet: Adding Multiple Tasks to a Single Network by Iterative Pruning\n",
        "##CVPR 2018\n",
        "#### Arun Mallya and Svetlana Lazebnik\n",
        "\n",
        "Inspired by network pruning techniques, PackNet exploits redundancies in large deep networks to free up parameters that can then be employed to learn new tasks. By performing iterative pruning and network re-training, PackNet is able to sequentially “pack” multiple tasks into a single network. To do that, after finding $n$<sup>th</sup> pack, it freezes the weights and assigns that \"pack\" as a subnetwork of $task$ $n$. The drawback here is, PackNet forces next tasks to use previously fixed and pretrained connections. It is called a biased transfer. However, it is one of the first approaches that pave the way for trainable subnetworks.\n",
        "\n",
        "In the figure, white circles represents available neurons in the backbone while bold circles indicates neurons that are already occupied in another pack and fixed that is why in the next pack selection these neurons have to be used whether they are relevant with the task at hand or not.\n",
        "\n",
        "* **Masking Method**: Train the all backbone then remove 50% or 75% of the connections based on weights' absolute magnitude. Re-train.\n",
        "* **Mask Selection**: Assumed that task identity is given in both the training and the testing stage.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/packnet.png?raw=true\" width=800>\n",
        "</p>"
      ],
      "metadata": {
        "id": "jM5KlF8g1zLu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Piggyback: Adapting a Single Network to Multiple Tasks by Learning to Mask Weights\n",
        "## ECCV 2018\n",
        "#### Arun Mallya, Dillon Davis, and Svetlana Lazebnik\n",
        "\n",
        "Piggyback questions whether the weights of a network have to be changed at all. It suggest we might get reasonable results with just selectively masking, or setting certain weights to 0, while keeping the rest of the weights the same as before. \n",
        "\n",
        "Based on this idea, Piggyback learns how to mask weights of an existing pretrained network (e.g. VGG-16) for obtaining good performance on a new task, as shown in the Figure. Binary masks that take values in {0, 1} are learned and stored after each task. And to learn those binary masks, Piggyback trains the mask since the model itself is already pretrained. To do that, it first starts with real-valued masks and uses the loss of the pretrained model in order to update real-valued masks. Finally, it applies threshold function take make it binary.\n",
        "\n",
        "* **Masking Method**: Mask the pretrained model based on learnable real-valued masks which are then converted to the binary mask with thresholding.\n",
        "* **Mask Selection**: Assumed that task identity is given in both the training and the testing stage.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/piggyback.png?raw=true\" width=700>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "DK7rElqguenW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SupSup: Supermasks in Superposition\n",
        "## NeurIPS 2020\n",
        "#### Mitchell Wortsman et al.\n",
        "\n",
        "Supermasks in Superposition (SupSup) model uses a randomly initialized, fixed base network and for each task finds a subnetwork (supermask). If task identity is given at test time, the correct subnetwork can be retrieved with minimal memory usage. If not provided, SupSup can infer the task using gradient-based optimization to find a linear superposition of learned supermasks which minimizes the output entropy. Authors experimentally find that a single gradient step is often sufficient to identify the correct mask, even among 2500 tasks. Hence, SupSup is suitable for class-IL.\n",
        "\n",
        "\n",
        "During training, SupSup learns a separate supermask (subnetwork) for each task. At inference time, SupSup can infer task identity by superimposing all supermasks. Ideally, appropriate supermask for a given task should exhibit a confident output distribution (i.e. low entropy).\n",
        "\n",
        "* **Masking Method**: Deconstructing lottery tickets: Zeros, signs, and the supermask.\n",
        "* **Mask Selection**: Try all the supermasks, return mask with a lowest entropy\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/supsup.png?raw=true\" width=800>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "ItpNEe7__RUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SpaceNet: Make Free Space for Continual Learning\n",
        "## Neurocomputing 2021\n",
        "#### Ghada Sokar, Decebal Constantin Mocanu, and Mykola Pechenizkiy\n",
        "\n",
        "SpaceNet trains sparse deep neural networks from scratch to have compact number of neurons for each task. When the model faces a new task, new sparse connections are randomly allocated between a selected number of neurons in each layer. At the end of the training, the initial distribution of the connections changes and connections that are important for that task group together.\n",
        "\n",
        "The most important neurons for a specific task are reserved to be specific to this task, and will not be seen by the following tasks and will freeze. However, other neurons that are somehow important or not important at all, will continue to be shared between the tasks. For example, in the figure, fully filled circles represent the neurons that are most important and become specific for $task$ $t$, where partially filled ones are less important and could be shared by other tasks. Multiple colored circles represent the neurons that are used by multiple tasks. After learning $task$ $t$, the corresponding weights are kept fixed.\n",
        "\n",
        "For convolutional neural networks, SpaceNet performs a coarse manner in drop and grow phases to impose structured sparsity instead of irregular sparsity. In particular, in the drop phase, coarse removal for the whole kernel is applied instead of removing scalar weights. Similarly, in the grow phase, the whole connections of a kernel are added instead of adding single weights. The likelihood of adding a kernel between two feature maps is inversely proportional to their significance, similar to multilayer perceptron networks. \n",
        "\n",
        "* **Masking Method**: A fraction $r$ of the sparse connections in each layer is dynamically changed based on the importance of the connections and neurons in that layer. Connection importance is estimated by its contribution to the change in the loss function. The first-order Taylor approximation is used to approximate the change in loss during one training iteration $i$. Growing and Dropping the connection is applied based on the importance score.\n",
        "\n",
        "* **Mask Selection**: Use the whole network structure, no masking.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/spacenet.png?raw=true\" width=700>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "0W2HuqQfIoJ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avoiding Forgetting and Allowing Forward Transfer in Continual Learning via Sparse Networks: AFAF\n",
        "## ECML 2022\n",
        "#### Ghada Sokar, Decebal Constantin Mocanu and Mykola Pechenizkiy\n",
        "\n",
        "This paper improves SpaceNet by allowing knowledge transfer between tasks which in the end increased the overall accuracy. It is still based on structured pruning (neuron pruning in FCN and channel pruning in CNN) while dynamically trains the sparse network. To enable the knowlege transfer between tasks, some new hyperparameters were introduced. Layers from $l$ = 1 up to but excluding layer $l$<sub>reuse</sub>, a hyperparameter, remains unchanged. To *selectively transfer* the relevant knowledge, for each layer l ≥ $l$<sub>reuse</sub>, they identify a set of candidate neurons R<sup>c</sup><sub>l</sub> that has a high potential of being useful when “reused” in learning class $c$ in a new task $t$. This high potential is determined based on the neuron activation. If the neuron higly activated for the new task then it is assumed that this neuron has a high potential to transfer knowledge. Here, they introduced another hyperparameter κ to select number of neurons to reuse per layer. Finally, starting from $l$<sub>reuse</sub>, new sparse connections are added to the network to learn patterns that are specific to new task $t$."
      ],
      "metadata": {
        "id": "y6T9swsu2yAB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Prune-and-Select (CP&S): Class-Incremental Learning with specialized subnetworks\n",
        "##*2022*\n",
        "#### Aleksandr Dekhovich, David M.J. Tax, Marcel H.F. Sluiter, and Miguel A. Bessa\n",
        "\n",
        "During training, Continual-Prune-and-Select (CP&S) finds a subnetwork within the DNN that is responsible for solving a given $task$ $t$. \n",
        "\n",
        "A new task is learned by training available neuronal connections (previously untrained) of the DNN to create a new subnetwork which can include previously trained connections belonging to other subnetwork(s) but those will not be updated. In other words, previously trained connections can be shared between tasks yet cannot be updated.\n",
        "\n",
        "Then, during inference, CP&S selects the correct subnetwork to make predictions for that task.  This enables to eliminate catastrophic forgetting by creating specialized regions in the DNN that do not conflict with each other while still allowing knowledge transfer across them. \n",
        "\n",
        "\n",
        "* **Masking Method**: NNrelief\n",
        "* **Mask Selection**: Try all the subnetworks, return mask with a lowest entropy (*maximum output response*)"
      ],
      "metadata": {
        "id": "y40AC2v9facf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Forget-free Continual Learning with Winning Subnetworks\n",
        "## ICML 2022\n",
        "#### Haeyong Kang et al.\n",
        "\n",
        "For each task, the WSN sequentially learns and chooses the best subnetwork. Specifically, WSN jointly learns the model weights and task-adaptive binary masks that pertaining to subnetworks associated with each task. It also attemps to select a small set of weights to be activated *(winning ticket)* by reusing weights of the prior subnetworks.\n",
        "\n",
        "It updates only the weights that have not been trained on the previous tasks. After training for each task, the model freezes the subnetwork parameters. Therefore, WSN is also immune to the catastrophic forgetting by design.\n",
        "\n",
        "For example, in the figure, $task$ $t-1$ is learned by a orange subnetwork. In the $task$ $t$, we can still use the orange weights while doing a forward pass yet we cannot use them on the backward pass. It is only allowed for unassigned weights.\n",
        "\n",
        "Its pruning (masking) approach a bit different though. WSN's network contains two different parameters: one for learning with parameters θ and, one for masking the network(θ) with parameters $s$. Based on the weight scoring parameters ($s$), $c$% weights are selected where $c$ is the target layerwise capacity ratio in %. Then the top c% are assigned to 1 and remainings are assigned to 0. This approach indirectly applies the masking to the main network with parameters θ. To update the weight scores $s$, loss of the main network (θ) is used in its backward pass.\n",
        "\n",
        "\n",
        "* **Masking Method**: WSN tries to find best subnetwork by selecting the $c$% weights with respect to learnable weight scores $s$, where $c$ is the target layerwise capacity ratio in %.\n",
        "\n",
        "* **Mask Selection**: Assumed that task identity is given in both the training and the testing stage.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/winningsubnetworks.png?raw=true\" width=700>\n",
        "</p>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CyCUBJLNCCXr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lifelong Learning with Dynamically Expandable Networks (DEN)\n",
        "##ICLR 2018\n",
        "#### Jaehong Yoon, Eunho Yang, Jeongtae Lee and Sung Ju Hwang\n",
        "\n",
        "DEN selectively retrains the old network, expands its capacity when necessary, and thus dynamically deciding its optimal capacity as it trains on. DEN consists of 3 steps:\n",
        "1. Train the network with $L$<sub>1</sub> regularization for $task$ $t$ to create some sparsity in the network. Then, train the network with $L$<sub>1</sub> regularization for $task$ $t$+$1$ again while only considering the remaining parameters this time.\n",
        "2. If remaining parameters is not enough (𝕃oss ≥ τ) to learn $task$ $t$+$1$, then expand the network in a top-down manner, still eliminating any unnecessary neurons by $L$<sub>1</sub> regularization which force to make it sparse.\n",
        "3. If parameters shifted too much from their inital values (𝕡 ≥ σ), then duplicate the weights. After this duplication of the neurons, the network needs to train the weights again since split changes the overall structure. However, in practice this secondary training usually converges fast due to the reasonable parameter initialization from the initial training.\n",
        "\n",
        "    (𝕡 = ℓ<sub>2</sub>-distance between the incoming weights at $t$-$1$ and at $t$)\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/den.png?raw=true\" width=800>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "wZ5vFhfOJtMS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Squeeze-and-Excitation Networks\n",
        "##CVPR 2018\n",
        "#### Jie Hu, Li Shen and Gang Sun\n",
        "\n",
        "“Squeeze-and- Excitation” (SE) blocks try to improve the representational power of a network by explicitly modelling the interdependencies between the channels of its convolutional features. To achieve this, it uses the global information to selectively emphasise informative features while suppressing less useful ones.\n",
        "\n",
        "The basic structure of the SE block is illustrated in figure. For any given transformation (e.g. a convolution or a set of convolutions), in features are first passed through a *squeeze* operation. It aggregates the feature maps across spatial dimensions to produce a channel descriptor with global pooling (CxHxW -> Cx1x1). \n",
        "\n",
        "This is followed by an *excitation* operation, in which channel-specific activations obtained by sigmoid function are learned for each channel. In features are then reweighted by corespondent channel-specific activations. At the end, it helps to consider important channels (or feature maps) heavily than the others for a given input set with a such simple attention mechanism.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/squeezeandexcitation.png?raw=true\" width=800>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "Ar7TVA653giG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional networks with adaptive inference graphs\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "hV7YeaZK2UsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Powerpropagation: A sparsity inducing weight reparameterisation\n",
        "##NeurIPS 2021\n",
        "#### Jonathan Schwarz et al.\n",
        "\n",
        "A weight-parameterization method for neural networks called powerpropagation produces models that are inherently sparse. Exploiting the behaviour of gradient descent, Powerprop gives rise to weight updates exhibiting a “rich get richer” dynamic by leaving low-magnitude parameters largely unaffected by learning. As a result, models trained in this way perform similarly but have a distribution with a noticeably higher density at zero, enabling the safe pruning of more parameters.\n",
        "\n",
        "In the forward pass of a neural networks, raise the parameters of the model to the α-th power (where α > 1) while preserving the sign. Parameters that are raised to α − 1 will appear in the gradient computation, scaling the usual update. Because of this, larger magnitude parameters receive larger gradient updates than smaller magnitude parameters do, resulting in the previously mentioned \"rich get richer\" phenomenon.\n",
        "\n",
        "In a simple formulation where w = v|v|<sup>α−1</sup>, for any arbitrary power α ≥ 1 we preserved the sign of v so that it can still represent both negative and positive values. For α = 1 this recovers the standard backpropagation setting. For α ≥ 1, updates in the weights are naturally obtained by the standard backpropagation but with enhanced gradients to enforce \"rich get richer\" phenomenon.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/powerprop.png?raw=true\" width=900>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "dBz0OgUfCO-g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning without Forgetting\n",
        "## TPAMI 2017\n",
        "#### Zhizhong Li and Derek Hoiem\n",
        "\n",
        "Learning without Forgetting (LwF) approach could be seen as a combination of Distillation Networks and fine-tuning. Fine-tuning modifies the parameters of an existing CNN to train a new task. A small learning rate is often used, and sometimes part of the network is frozen to prevent overfitting. Distillation Networks helps simpler networks to return more reasonable outputs by providing additional info from the input data.\n",
        "\n",
        "In LwF, let number of tasks is equal to $t$ and let tasks are defined in a class-incremental manner. Then each previous task's network will become a distillation network of the following task. (e.g. network of the $task$ $t-1$ will be assigned as a distillation network of $task$ $t$.) This distillation network will guide the main network of the current task while fine tuning so that it will not forget the previously learned tasks:\n",
        "\n",
        "0. Add $c$ number of nodes to the classifier of old network (distillation network) from $task$ $t-1$ to create new network for $task$ $t$.\n",
        "1. Forward input of the $task$ $t$ to both networks.\n",
        "2. Name logits of the distillation as *soft targets* and classes of the input of $task$ $t$ as *hard targets*.\n",
        "3. Compare hard targets with the predicted labels to calculate $cross$-$entropy$ $loss$.\n",
        "4. Compare soft targets with logits of the new network to calculate $distillation$ $loss$. (ignore newly added $c$  number of node(s) - in other words, consider only old classes which should not be forget)\n",
        "5. Initiate backbropagation with **Loss** so that new network will be enforced to learn new task while preseving the old taks by distilled information where\n",
        "\n",
        "    **Loss** = $cross$-$entropy$ $loss$ + λ*$distillation$ $loss$.\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/lwf.png?raw=true\" width=500>\n",
        "</p>"
      ],
      "metadata": {
        "id": "QY9S1-Vb_iQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overcoming catastrophic forgetting in neural networks\n",
        "## PNAS 2017\n",
        "#### James Kirkpatrick et al.\n",
        "\n",
        "This paper developed an algorithm analogous to synaptic consolidation for artificial neural networks, which is referred as elastic weight consolidation (EWC). This algorithm slows down learning (or changing) on certain weights based on how important they are to previously seen tasks. This importance is called Fisher information matrix $F$ and it has three key properties: (i) It is equivalent to the second derivative of the loss near a minimum, (ii) it can be computed from first-order derivatives alone and is thus easy to calculate even for large models, and (iii) it is guaranteed to be positive semidefinite.\n",
        "\n",
        "Overall, the loss function that we try to minimize in EWC is:\n",
        "\n",
        "$L$(θ) = $L$<sub>B</sub>(θ) + ∑ λ / 2 * $F$<sub>i</sub>(θ<sub>i</sub> + θ<sub>A,i</sub>)<sup>2</sup> \n",
        "\n",
        "where\n",
        "\n",
        "$L$<sub>B</sub>(θ): is the loss for latter task B only,\n",
        "\n",
        "λ: sets how important the old task A is compared with the new one and,\n",
        "\n",
        "i: labels each parameter.\n",
        "\n",
        "EWC will attempt to keep the network parameters close to the learned parameters of both tasks A and B when switching to a third task. This can be enforced either with two separate penalties or as one by noting that the sum of two quadratic penal- ties is itself a quadratic penalty.\n",
        "\n",
        "In the figure, after learning the first task, the parameters θ<sub>A</sub><sup>*</sup> were obtained. If we take gradient steps according to task B alone (blue arrow), we will minimize the loss of task B but destroy what we have learned for task A. However, if we impose an excessive amount of limitation by assigning the same coefficient to each weight (green arrow), we can only recall task A at the risk of failing to learn task B. EWC, on the other hand, clearly calculates how important weights are for job A in order to discover a solution for task B without suffering a major loss on task A (red arrow).\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/ewc.png?raw=true\" width=450>\n",
        "</p>"
      ],
      "metadata": {
        "id": "ylX87elZS-CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Learning Through Synaptic Intelligence\n",
        "## ICML 2017\n",
        "#### Friedemann Zenke, Ben Poole and Surya Ganguli\n",
        "\n",
        "Synaptic Intelligence (SI) accumulates task relevant information over time, and exploits this information to store new memories without forgetting old ones. To do that, each individual synapse is measured with a local measure of “importance” in solving tasks that the network has been trained on in the past For brevity, the term “synapse” used synonymously with the term “parameter”, which includes weights and biases between layers.\n",
        "\n",
        "When training on a new task, SI penalizes change in the important parameters to avoid old memories from being over-written. To that end, SI calculates each parameters' contribution to the loss function by slowly changing each parameter. If this small change in parameter affects the loss heavily, then this means that parameter plays an important role for the task. It should be not be updated in order to preserve old knowledge. Otherwise, if a small change in parameter doesnt affects the loss at all, then that parameter is not crucial for the task. Hence, it can easily be updated in the next task to learn that new task.\n",
        "\n",
        "**Importance** = *Parameter's contribution to the loss function* = **Gradient of the parameter**"
      ],
      "metadata": {
        "id": "dCQi7IyNz_hL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradmax: Growing neural networks using gradient information\n",
        "## ICLR 2022\n",
        "#### Utku Evci, Bart van Merrienboer, Thomas Unterthiner, Max Vladymyrov, Fabian Pedregosa\n",
        "\n",
        "Gradmax aims to grow the network architecture without costly retraining. It adds new neurons during training without impacting what is already learned, while improving the training dynamics by maximizing the gradients of the new weights and efficiently find the optimal initialization by means of the singular value decomposition (SVD). \n",
        "\n",
        "It starts with a small seed architecture. Then over the course of the training, new neurons are added to the seed architecture: either increasing the width of the existing layers or creating new layers. \n",
        "\n",
        "In the illustration of Gradmax, growing neurons require initializing incoming W<sub>l</sub><sup>new</sup> and outgoing W<sub>l+1</sub><sup>new</sup> weights for the new neuron. GradMax sets incoming weights W<sub>l</sub><sup>new</sup> to zero (dashed lines) in order to keep the output unchanged so that backprop will not impact the currently learned weights. It initializes outgoing weights W<sub>l+1</sub><sup>new</sup> using SVD. This maximizes the gradients on the incoming weights with the aim of accelerating training.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/gradmax.png?raw=true\" width=700>\n",
        "</p>"
      ],
      "metadata": {
        "id": "JZHwHfKY2dAC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning\n",
        "##ICML 2022\n",
        "#### Utku Evci, Vincent Dumoulin, Hugo Larochelle and Michael C. Mozer\n",
        "\n",
        "Head-to-Toe probing (Head2Toe) selects features from all layers of the source model to train a classification head for the target domain. It aims to replace traditional transfer learning approach by assuming relevant feature maps can occur anywhere in the network instead of the last layer.\n",
        "\n",
        "It connects outputs (feature maps) of the all layers with classifier head. And then applies Lasso Regulizer to select only relevant features for the classifier. Since Lasso force connections to be zero if they are irrevelant only important featur maps would contribute to the output.\n",
        "\n",
        "Head2Toe matches performance obtained with fine-tuning on average while reducing training and storage cost, but critically, for out-of-distribution transfer, Head2Toe outperforms fine-tuning.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/papers/head2toe.png?raw=true\" width=350>\n",
        "</p>"
      ],
      "metadata": {
        "id": "GvZdRgS89dpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SparCL: Sparse Continual Learning on the Edge"
      ],
      "metadata": {
        "id": "fco10XwjsRxF"
      }
    }
  ]
}
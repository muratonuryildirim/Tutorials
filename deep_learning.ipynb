{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO0V6wzte+1HLhNKkwbLi6G"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning \n",
        "\n",
        "Deep learning (DL) is a branch of machine learning (ML) that employs algorithms inspired by the neural network of the brain which is called an artificial neural network (ANN). DL became more popular over time because it turned out to be more capable of processing raw data while ML requires some preprocessing steps to construct a viable model. But what is machine learning then?\n",
        "\n",
        "### Machine Learning\n",
        "\n",
        "Machine learning is programming computers to perform a task based on experience (examples) without giving explicit instructions. During this process, we should try to minimize the error so that the developed model can perform well.\n",
        "\n",
        "In a more mathematical way, we want to learn a function (model) with its parameters that produces the right output for a given input:\n",
        "\n",
        "f<sub>$\\theta$</sub>(x) = y\n",
        "\n",
        "argmin<sub>$\\theta$</sub> $\\epsilon$(f<sub>$\\theta$</sub>(x))\n",
        "\n",
        "#### How it is different than traditional programming?\n",
        "\n",
        "In traditional programming, systems are coded to react based on specific instructions. However, it is not the case in machine learning. The concept of machine learning or artificial intelligence advocates that giving specific instructions for each distinct situation would not be possible. Instead, the program or system should be able to learn by the provided data (examples) itself and react based on this knowledge which is a learned model function. \n",
        "\n",
        "#### How it is different than the statistics?\n",
        "Both of them aim to make predictions of natural phenomena but there are some differences. Statistics help humans understand the world by assuming that data that occurs in the universe is understandable by humans. Machine learning, on the other hand, has no such specific effort or purpose to explain how the world works besides assuming that the data generation process is unknown.\n",
        "\n",
        "#### Types of machine learning\n",
        "\n",
        "* Supervised Learning: learning a model f<sub>$\\theta$</sub>(x) from labeled data (X,y) : Given a new input X, predict the right output y.\n",
        "Example: Given examples of different houses and apartments' attributes (X), predict the price (y) of the unseen house or apartment.\n",
        "\n",
        "* Unsupervised Learning: learning a model f<sub>$\\theta$</sub>(x) from unlabeled data (X) to explore the structure of the data and extract meaningful information.\n",
        "Example: Given inputs X, find which ones are special, similar, anomalous.\n",
        "\n",
        "* Semi-Supervised Learning: learning a model f<sub>$\\theta$</sub>(x) from a few labeled and many unlabeled data.\n",
        "\n",
        "* Reinforcement Learning: creating an agent that improves its performance over time as a result of interactions with the environment."
      ],
      "metadata": {
        "id": "WtLsUp9edUlp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Artificial Neural Networks\n",
        "\n",
        "An artificial neural network (ANN) is a computing system consists of a collection of connected components known as *neurons* that are grouped into a structure called *layer*. Each neuron-to-neuron connection sends a signal from one neuron to the next. The signal is processed by the receiving neuron, which then sends messages to downstream neurons in the network. Note that neurons are also known as *nodes*.\n",
        "\n",
        "A very basic ANN architecture is nothing but logistic regression which is the fundamental algorithm of the ML. If you recall what logistics regression does to make the classification:\n",
        "\n",
        "* it creates a linear model using the dot product of input vector **x** and weight vector **w** and adds up a bias term *w<sub>0</sub>*.\n",
        "* then the linear model *w<sub>0</sub>* + **wx** push trough to a sigmoid function to get a prediction between 0 and 1.\n",
        "* as the last step, it uses a log loss a.k.a. cross-entropy as a loss function, and gradient descent as an optimizer to learn the weights.\n",
        "\n",
        "Logistic Regression in mathematical notation:\n",
        "\n",
        "y(x)= sigmoid(*w<sub>0</sub>* + **wx**) = (*w<sub>0</sub>* + *w<sub>1</sub>**x<sub>1</sub>* + *w<sub>2</sub>**x<sub>3</sub>* + ... + *w<sub>p</sub>**x<sub>p</sub>*) where p is the number of features.\n",
        "\n",
        "Logistic Regression in schematic representation:\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "(schema)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "\n",
        "When the schematic representation is checked closely, one can see that logistic regression is nothing more than 1-layer neural network. It is actually the building block of the ANNs! Large ANN architectures can be easily created if the logistic regression blocks are combined together. In other words, **Artificial neural networks are nothing but a combination of logistic regression blocks.**\n",
        "\n",
        "To call it ANN, adding one or more layers that are named *hidden layers* is enough. Weights between nodes in different layers form a weight matrix W<sup>ℓ</sup> per layer ℓ.\n",
        "\n",
        "And each layer is constructed in a 2-step process:\n",
        "\n",
        "1. the dot product (*z*) of weight matrix W<sup>ℓ</sup> and input matrix **x**  is calculated.  \n",
        "2. *z* is passed through the non-linear activation function such as the sigmoid function and the output **a** is called activation.\n",
        "\n",
        "a<sup>0</sup> = **x**\n",
        "\n",
        "a<sup>1</sup> = f(z) = f(W<sup>1</sup>a<sup>0</sup> + w<sub>0</sub><sup>1</sup>)\n",
        "\n",
        "a<sup>2</sup> = f(z) = f(W<sup>2</sup>a<sup>1</sup> + w<sub>0</sub><sup>2</sup>)\n",
        "\n",
        "where f is the activation function and a<sup>0</sup> is just the input data. Of course depending on how many input and output nodes, we will end up with different numbers of weight matrices and activations. \n",
        "\n",
        "In a typical ANN, it is expected to have more layers. More layers mean having more flexibility but difficulty to train which can require lots of data and time, depending on the computational power. \n",
        "\n",
        "Moreover, having more layers in the neural network makes it trickier to train since once you push data to the network, the model should do a backpropagation to update its weights and, the more layers the model have the more signal of the backpropagation will get lost throughout the layers.\n",
        "\n",
        "But then *why more layers?* The model can consist of one hidden layer with thousand nodes instead of 100 layers each has 10 nodes after all. It turns out that training more layers with a smaller number of nodes is much easier than the other one. This is because of the term '*compositionality*' which means a hard problem can be composed into smaller and simpler subproblems and this is what the layers do. The task at hand gets simpler and simpler after each layer while the model learns the representation of the provided data better after each layer as well so that it can solve the task. In other words, layers are the *filters* that simplify the representation of the task. To solve more complex problems, one can even provide an N-dimensional weight tensor which will cause N-dimensional outputs instead of a single weight matrix.\n",
        "\n",
        "The magical thing in the ANN learning process is that although we do not give specific directives about what the model should look for, the model learns to decide what is important by looking at the data given to it and to represent it in the best possible way.\n",
        "\n",
        "### Types of Neural Networks\n",
        "\n",
        "(figure)\n",
        "\n",
        "* Feed Forward Networks: Typical structure of the neural networks. They are usually fully connected which means all nodes have a connection with each other.\n",
        "\n",
        "* Convolutional Neural Networks:  This one has a special neuron which does convolution and usually used for images or continuous data such as like a sound or even text sometimes. Because it assumes that the necessary information about one point should be adjacent or close to that point we are interested in. So, it is almost tailored for the images. It transforms input image to some abstract representation and then we pass that abstract representation data to a few dense layers so we can predict an output y. In other words, CNNs extract the patterns of the complex data by assuming that necessary information should be nearby.\n",
        "\n",
        "* Recurrent Neural Networks: In this kind of neural networks, neurons not only have connections with the neurons of the next layers but they also have connections to themselves. Hence, they have an extra input which is basically their output some time ago. This extra operation provides some kind of short memory to a neural network. It can what able to keep in mind what it has seen before. This networks mostly suitable for sequencial data such as speech and time-series. RNNs assumes that necessary information must have been occurred in near-past.\n",
        "\n",
        "* Residual Neural Networks: This networks have skip connections so they don't only connect the next layer but also they connect to the further layers. That is helpful when a longer and bigger networks have been created since some patterns may get lost to  between the layers. Hence, skip connections help to remember those patterns through the network which allows to build deeper neural networks.\n",
        "\n",
        "* Encoders: It is designed to represent the provided data with as\n",
        "few nodes as possible. In other words, encoders try to find the best low dimensional representations (embeddings) of the data. It can be used for both dimension reduction and data generation.\n"
      ],
      "metadata": {
        "id": "tjBEFx5sL3qk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a Neural Network\n",
        "\n",
        "### Components of the Neural Network\n",
        "To train a network 5 different components need to be decided. **Number of layers and nodes** in the network which constructs the main body, **activation functions** which lets the main body to capture non-linear relationships in the data, **loss function** that measures how good or bad the predictions made by the network, randomly initialized **weights**, and **optimizer** which updates the weights based on the loss value to perform better. \n",
        "\n",
        "### Steps to Train the Network\n",
        "1. **Forward Pass**: This is basically the prediction step based on the current model and it can be illustrate as *activation_function*( X . W + b )\n",
        "\n",
        "    * Divide data into smaller batches.\n",
        "    * Give batch of instances to first layer\n",
        "    * Take the dot product of the batch and the 1st weight matrix.\n",
        "    * Sum the bias values with that matrix.\n",
        "    * Use activation function on the last matrix that is calculated to find the out matrix which will be the new input for the following layer.\n",
        "    * Repeat the steps until reaching to the last layer of the network.\n",
        "\n",
        "2. **Backward Pass**: This is where optimizer updates the weights of the model to predict better in the next forward pass. It can be illustrate as W<sub>i+1</sub> = W<sub>i</sub> - ∂ Loss(x, W<sub>i</sub>) \\* η. The most effective algorithm behind this optimization process is *Mini Batch Stochastic Gradient Descent* because the data that is divided into small batches in the forward pass gives enough signal to learn the weights. It is also much faster than giving the entire dataset to network which usually tends to overfit. \n",
        "\n",
        "    * Select the model's *learning rate*. It is usually a small number that keeps the change in weight matrix under control.\n",
        "    * Calculate gradient with respect to layer's weights.\n",
        "    * Update the weights of the layer by subtracting the gradient that is multiplied with *learning rate* from the corresponding weight matrix.\n",
        "    * Repeat the steps until reaching to the first layer of the network.\n"
      ],
      "metadata": {
        "id": "AupDUqO4yMRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Functions\n",
        "\n",
        "We have learned how to train neural networks in the previous section and now, we can talk about different activation functions in the neural networks and also how to initialize the weights of the neural network.\n",
        "\n",
        "Historically the first activation function that was used a lot was *sigmoid funtion*. It carries all the input values between 0 and 1 which makes a lot of sense since we already want to smooth the outputs into some values between 0 and 1. However, it turned out that *sigmoid function* does not always converge well. That is because the gradient descent works better with the activations that are around zero. Hence, *tanh function* is introduced to the neural networks to have better convergence during the training process. It is basically a sigmoid function that carries all the input values between 1 and -1 instead of 0 and 1 so that most of the activations will be end up around the zero. The only downside of the *tanh function* is being expensive to compute. That is why there is another activation function called *Rectified linear Unit (ReLU)*. It basically makes 0 all the negative values and leave the rest of the inputs as it is. ReLU is not really a smooth function and also it cannot be differentiable at 0 but it is much more faster and the rest of the functions. Finally, there is a *Leaky ReLU* which is quite popular. It is a *ReLU* but instead of making all the negative weights 0, it makes them really close to zero with multiplying those weight with 0.01. We will see why this matters. \n",
        "\n",
        "\n",
        "The derivative of the activation function actually defines how large the gradients are in the network: if the derivative of the activation function happens to be zero then the gradient will be equal to zero which means there will be no update on the weights and no learning is happening. Moreover, we cannot update the weights of the deep networks if the derivative of the activation function is very close to the zero because the gradients of the previous layers will be drastically reduced which is known as vanishing gradients. So, we definitely want to avoid these situations. When we check the derivate of the *tanh*, it is non-zero around zero but other than that the derivative is almost equal to zero which means the gradient will disappear quite fast and it will not be ideal for the deep networks. *ReLU* can be much more better activation function to create deep networks. The derivative of the ReLU is equal to the 1 when the weights are greater than 0. The only problem with the *ReLU* is if the model has very negative weights then those weight cannot be updated since the weight stucks the zero in the derative of the *ReLU*. This issue known as *dying ReLU*. Finally, the *Leaky ReLU* could help weights to not to stuck in that danger zone even if the weights have very negative values. That is why multiplying the negative weight with 0.01 matters instead of making them all 0. \n",
        "\n"
      ],
      "metadata": {
        "id": "d6CB5NUlFtPu"
      }
    }
  ]
}
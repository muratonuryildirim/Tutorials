{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPS5TYGHA/pXLshYnXdL5NU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning \n",
        "\n",
        "Machine Learning (ML) is a field of study that gives computers the ability to learn without being explicitly programmed.\n",
        "\n",
        "Its also more formally and mathematically explained by T.M. Mitchell in 1997 as \"It is a computer program that is said to learn from experience $E$ with respect to some class of tasks $T$ and performance mesaure $P$ if its performance at tasks in $T$, as measured by $P$ improves with experience $E$.\"\n",
        "\n",
        "And its flow was defined as \"The result of running the machine learning algorithm can be expressed as a **function**. The precise form of the function is determined during the **training** phase, also known as the **learning** phase on the basis of the training data. Once the model is trained it can then determine the identity of new data which are set to comprise a **test set** the ability to categorize correctly new examples that defer from those used for training is known as **generalization**.\" in 2006 by C.M. Bishop.\n",
        "\n",
        "How about the goal of the Machine Learning? \"It is (i) choosing the right parameters to reduce training set error, then (ii) making the gap between training and test error as small as possible.\" I. Goodfellow, 2016.\n",
        "\n",
        "These two point are highly related with the term \"*bias variance trade-off*\". If the model has high bias then it tends to **underfit** to training data, resulting high error and low performance on training data. On the other hand, If the model has high variance then it tends to **overfit** to training data, resulting high error and low performance on test data while performing excellent on training data. Hence, a model that balances the bias-variance trade-off would always be preferable since the main goal of the ML is **generalization** over the data.\n",
        "\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/model_data_overtime.png?raw=true\" width=750>\n",
        "\n",
        "In the development of ML, there was a big emphasis on presenting better result on certain benchmarks or datasets such as ImageNet. In return, models and compute are got bigger and more accurate over time. On the other hand, the benchmarks and datasets are remained same over that time. The success of the models was measured by giving high accuracy rates on a fixed datasets. \n",
        "\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/catastrophic_forgetting.png?raw=true\" width=610> \n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/ml_overconfident.png?raw=true\" width=325>\n",
        "\n",
        "However, in the real case scenario, data is alive and its change over time. When the assumption of immutably fixed datasets is removed, those large and computationally heavy models no longer looked good at all. It turns out machine learning models tended to forget really quickly. When models were fed with new information upon learning, they were forgetting the older information they have learned, even though they were capable to learn both information if that information provided together at the same time or in the same task.\n",
        "\n",
        "\n",
        "But why this is important? The world is open and dynamic yet ML is not capable to keep pace with this dynamism and it is also misleadingly overconfident. That is why constructing ML models that can learn over time continually while not forgetting the older tasks is important, and referred as **continual learning** or **lifelong learning** in general.\n",
        "\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/cleva.png?raw=true\" width=900>\n"
      ],
      "metadata": {
        "id": "o1EYxg3aljZm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First Lights of Continual Learning: Transfer Learning\n",
        "\n",
        "Initially, each machine learning model was being used for a single task in which the model was trained. However, having a system that can  transfer the knowledge from previous task to solve new task would be more logical. With this motivation, it was discovered that while simpler and general patterns were learned in the first layers of artificial neural networks, more complex and specific patterns were learned in the later layers. Hence, the idea of transferring the knowledge become more concrete. \n",
        "\n",
        "Later, transfer learning proved its success with few-shot and zero-shot learning. The idea was using only very few traning data in the next task to exploit previous knowledge as much as possible. To do that, embedding spaces of the extracted features and activations were constucted. It turns out similar classes was ending up closer in those embedding spaces. Hence, with help of the transfer learning and embeddings, only few observations was become adequate to train the model and return high performance scores.\n",
        "\n",
        "Unfortunately, if the tasks were not that similar, transfer learning was failing because (i) model was not returning sufficient level of performance on $task$ $t$+$1$, (ii) while disturbing the old knowledge (forgetting) from $task$ $t$ which yields a incompetent model in the end for the all tasks."
      ],
      "metadata": {
        "id": "1-g5CRbL127l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Learning\n",
        "\n",
        "According to the traditional machine learning approach, it is assumed that data is already and always available. Hence, the model could work nicely if it is trained by the data on-hand. Recently, scientists and researchers realize that it is not the case. They, unfortunately, found that artificial neural networks have a tendency to forget. It is even illustrated that the networks forget previously learned information completely and abruptly upon learning new information which is called **catastrophic forgetting**.\n",
        "\n",
        "When the trained model is faced with an unfamiliar observation or task, it could confuse and lead to wrong answers. Therefore, it has been suggested that this static approach should be replaced with the more dynamic one so that machine learning models can adapt themselves to the new tasks or experiments.\n",
        "\n",
        "Continual Learning is the concept to learn a model for a large number of tasks sequentially without forgetting knowledge obtained from the preceding tasks, where the data in the old tasks are not available anymore during the training of the new ones.\n",
        "\n",
        "Tasks should have the following criteria to be continuous:\n",
        "\n",
        "* Data (and tasks) become available only during the time.\n",
        "* No access to previously encountered data.\n",
        "* Constant computational and memory resources (efficiency)\n",
        "* Incremental development of ever more complex knowledge and skills (scalability)\n",
        "\n",
        "There are other also other approaches that should not be confused with the continual learning to make the machine learning systems more dynamic and adaptive:\n",
        "\n",
        "* Multi-Task Learning\n",
        "* Meta-Learning / Learning to\n",
        "Learn\n",
        "* Transfer Learning & Domain\n",
        "Adaptation\n",
        "* Online / Streaming Learning \n",
        "\n",
        "To sum up, continual learning aims find a well-generalized model that remembers past concepts while learning the new concepts as well. It is not an easy job since the model should understand which biased information should be removed and which biased information should be preserved. Continual learning also assumes that there will be no access to previously encountered data and tasks. That assumption is important from two different aspects: First, in a continuous environment, the thing that is gone is gone. It can not exist again. Second, from the resource aspect, it is not possible to store all the previous information the model has encountered before. As an example, an average human consumes approximately 50 GB/s while observing stuff which makes ~30240 TB of data after only a week."
      ],
      "metadata": {
        "id": "sCJr1cAglFcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Catastrophic Forgetting\n",
        "\n",
        "Forgetting is something that is not just related to machines but it is part of the very nature of humans and the process of learning. It is actually important to forget the biased information or all information acquired over time in terms of effectiveness and efficiency. Models cannot remember everything and even if it remembers them it is just a waste of resource to remember things that are not useful to reach a particular goal.\n",
        "\n",
        "However, the level of forgetting is what matters here. The model should forget the unnecessary to generalize well yet it should retain the key information to 'learn'. The term **catastrophic forgetting**  means losing information completely and abruptly upon learning new information mostly due to the *gradient descent*. That is because, in the traditional approach, the model tries to minimize the loss **on a given task** which can be completely different for another task. Hence, continual learning shifts its standpoint a bit and it suggests that the model should try to minimize the loss **over the entire stream of task** which is represented as L<sub>$s$</sub>.\n",
        "\n",
        "The interesting and funny point here is this: the model could work perfectly if all the data at hand were fed to the model at once, yet when we split the data into smaller pieces and feed them into the model as a stream of tasks, the model is starting to forget. It is very remarkable to see how such a small change affects the performance of the model. However, it is not possible to have all the data at hand at once in a real case scenario which shows the motivation behind the continual learning and the significance of the catastrophic forgetting."
      ],
      "metadata": {
        "id": "RaQ-fnfCognu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Shifts in ML\n",
        "\n",
        "In the traditional offline machine learning, data and its distrubtuion is fixed while in the real world scanerio data is something dynamic and can change over time. To define these kind of changes researchers have come up with different terms:\n",
        "* Covariate Shift (shift happens in inputs): P(X) - same input returns same output but inputs are changed a bit.\n",
        "* Prior Probabilty Shift (shift happpens in output): P(y) - same input returns same output but frequency or the distribution of the output is changed a bit.\n",
        "* Concept Shift (shift happpens in output): P(y|X) - same input does not return same output anymore. Model should adapt itself - means retraining is necessary.\n",
        "\n",
        "The first two terms are classified under a virtual drift which is concerned by continual learning since the shifts we encounter is just a the result of a sample or task selection order (or bias). Therefore, in the virtual drift case, it should be assumed that all the previous examples or the previous observations are still valid and we need to accumulate knowledge over time. On the other hand, other term is classified under a real drift which mostly investigated by online learning and AutoML. To sum up, one should be able to identify the type of shift so that the necessary action can be taken.\n",
        "\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/datashift.png?raw=true\" width=600>\n",
        "\n"
      ],
      "metadata": {
        "id": "M40lWcEkePuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Assumptions in Continual Learning\n",
        "\n",
        "* *Shift is only virtual* which means forgetting is not needed only accumulation of knowledge would be enough.\n",
        "* *No conflicting evidence* that stands for one x value can be only \n",
        "valid for one y since we are modeling a mathematical function.\n",
        "* *Unbounded time between two experiences* which describes multiple time of training can possible for one experiment.\n",
        "* *Data processing valid in each experience* so that data in a given experience can be shuffled and processed freely."
      ],
      "metadata": {
        "id": "c5Rasq6nqmId"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Categorization of the Continual Learning Approaches\n",
        "\n",
        "Incremental learning aims to develop artificially intelligent\n",
        "systems that can continuously learn to address new tasks\n",
        "from new data while preserving knowledge learned from\n",
        "previously learned tasks. There are different incremental learning approaches in the literature.\n",
        "\n",
        "* **Task Incremental**: Models are informed about which task needs to be performed. This is the naive continual learning scenario since task identity is always provided. In this scenario it is possible to train models with task-specific components.\n",
        "\n",
        "* **Domain Incremental**: Identity of the task is not available during the test time. Models, however, only need to solve the task at hand. In other words, they are not required to infer which task it is. Typical examples of this scenario are protocols whereby the structure of the tasks is always the same, but the input-distribution is changing.\n",
        "\n",
        "* **Class Incremental**: The learner does not have access to any information about the task. It must be able to both solve each task seen so far and, at inference time, therefore must be able to distinguish between all classes from all tasks.\n",
        " Some argue that the concept of the class incremental learning is a bit deviated from reality since the structure of the concept does not allow seeing another observation from the same class which is not reasonable in most real case scenarios.\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continualmnist.png?raw=true' width=600 >\n",
        "\n",
        "To demonstrate the differences between the continual learning scenarios better, let's illustrate an example about MNIST dataset.\n",
        "\n",
        "*Task-IL*: With task given, is it the 1st or 2nd class? (e.g., 0 or 1)\n",
        "\n",
        "*Domain-IL*: With a unknown task, is it a 1st or 2nd class? (e.g., in [0, 2, 4, 6, 8] or in [1, 3, 5, 7, 9])\n",
        "\n",
        "*Class-IL*: With a unknown task, which digit is it? (i.e., choice from 0 to 9)\n"
      ],
      "metadata": {
        "id": "wm_JySrBz91-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Timeline of the Continual Learning\n",
        "\n",
        "### Past Focus\n",
        "The Task-Incremental approach was the initial step in the continual learning setup. It usually consists of a few big tasks. Scientists and researchers had worked on toy datasets to investigate the concepts in detail. The type of the task was mostly supervised and accuracy was the base metric.\n",
        "\n",
        "### Current Focus\n",
        "Even though the Task-Incremental approach still takes place, recently the direction shifted towards to Class-Incremental Learning. Instead of using a few big tasks and experiences, a larger number of experiments is preferred since it is more in line with its nature. However, people still tend to use supervised toy datasets with accuracy metrics.\n",
        "\n",
        "### Future Focus\n",
        "In the future, works based on more realistic datasets with diverse machine learning tasks such as unsupervised learning, and different performance metrics to evaluate are expected. Hence, more practical results about continual learning can be obtained. Besides that, of course, there is still a lot to investigate in the context of continual learning."
      ],
      "metadata": {
        "id": "bhBJn3Dc9V9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Alleviate Forgetting?\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/cl_mindmap.png?raw=true' width=450 >\n",
        "\n",
        "**1. Regularization-Based:** Identify relevant parameters for a task and make sure they do not change much while allowing other parameters to change freely. Or, make sure the input-output relationship remains the same. Therefore, a dilemma called stability-plasticity is arised. \n",
        "\n",
        "Stability means model has converged to a local minimum and it is wanted to stick to that point (not forgetting) and Plasticity means that the model is allowed to adapting itself (learning new task). Lets say, as in the figure, model is already converge to a local minimum in $task$ $t$ and then $task$ $t$+$1$ is added to the system. If we allow for too much plasticity or too much sensitivity then, it might learn task $t$+$1$ very well even for the sake of forgetting initial knowledge of $task$ $t$. On the other hand, too much stability makes the model untrainable for the next tasks, model would not forget the task though. Hence, with sweet spot between \n",
        "stability-plasticity dilemma, models will be able to put both tasks together to create some sort of trajectory that takes into account both of these minima and finds a solution that's adequate to both. \n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/stability_plasticity.png?raw=true' width=850 >\n",
        "\n",
        "*1.1. Elastic Weight Consolidation(EWC):* Applies stability to the parameters that are important for a given task. To measure the importance, it employes Fisher Information (natural gradient/second derivative of the loss).\n",
        "\n",
        "*1.2. Synaptic Intelligence(SI):* Applies stability to the parameters that are important for a given task. To measure the importance, it employes gradient by assuming that loss is well approximated by gradient of each parameter.\n",
        "\n",
        "Related but quite different idea would be the knowledge distillation to make sure that input-output relationship remains the same. Idea is not been formulated for continued learning initially but it suited well in this context.\n",
        "\n",
        "*1.3. Learning without Forgetting(LWF):* Assigns model of the $task$ $t$ as a teacher of a task $t$+$1$. Model $task$ $t$+$1$ (student) will be trained by both the teacher (to preserve old knowledge) and new task (to capture new information).\n",
        "\n"
      ],
      "metadata": {
        "id": "NGjJpajBmWJb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**2. Rehearsal-Based:** Stock subset of an old data and utilize it in the following tasks to not to forget (*Examplar Rehearsal*). Or, make use of a generative model to generate data of the old tasks (*Generative Rehearsal*). \n",
        "\n",
        "Initial studies such as *Incremental Classifier and Representation Learning(iCaRL)* tried to select few best examplars from the previous tasks so that it would not hurt the memory and the assumption of the continual learning. However, following studies such as *GDumb* showed that we dont even need to select few best examplars, simply choosing some random few examplars would maintain the knowledge for the seen tasks.\n",
        "\n",
        "One contradictory idea was rehearsing the raw input is not realistic and natural since a human does not store visuals itself but stores a memory or a representation. Hence, generating inputs was suggested instead of using raw input.\n",
        "\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/pseudo_rehearsal.png?raw=true' width=400 >\n",
        "\n",
        "From the perspective of a neuroscience, there are two models or two\n",
        "systems that interplay with each other in the brain: *Hippocampus* whose role is to have short-term adaptation and rapid learning of novel information, and *neocortex* which is responsible for slow learning, consolidating and building up the overlapping representations. Now, the idea of pseudo-rehearsal approach is comes from inteaction between those two. Once we sleep, in the RAM phase where a lot of activity in the brain observed, it's assumed that the patterns are being replayed to the *neocortex* from *hippocampus* so they can be consolidated into our memory. Hence, the ideas that we've observed during the day can be manifested more properly.\n",
        "\n",
        "Based on this, *deep generative replay* is developed. In this study, two models are employed: one for generating pseudo-rehearsal inputs and one for the actual task solving. It turned out this approach works equally when compared to initial studies if we have a strong and well designed generative model.\n"
      ],
      "metadata": {
        "id": "5uPplFi8JUk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Architecture-Based:** Use task-specific masks in an overparameterized models. Or, grow/expand the architecture. \n",
        "\n",
        "Architecture-Based assumes that catastrophic forgetting is a direct consequence of the overlap of distributed representations and can be reduced by reducing this overlap (Robert French, 1993).\n",
        "\n",
        "*3.1. Using task-specific sub-models in an overparameterized models.*\n",
        "\n",
        "It is also known as implicit perspective of a modular architecture. Main idea is identifying the subset of important parameters and subset of important connections in the neural network and creating sub-modules within a larger set.\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/hard_attention.png?raw=true' width=450 >\n",
        "\n",
        "\n",
        "More illustratively, as in the figure, we first capture the embedding of the input and multiply this embedding with binary gate which actually acts as a hard attention mechanism. When this process repeated over all the network we are ending up with a sub-module that is responsible for a given task.\n",
        "\n",
        "*3.2. Expanding the architecture.*\n",
        "\n",
        "It is also known as explicit perspective of a modular architecture. Instead of masking and finding subnetworks in a large network, we start from a small network and continiously add new parameters and increase the capacity of the model over a time if it is needed. For example, in the figure, we first try to decide which expert network would be suitable for a given input and send that input to a responsible expert network.This approach was the pioneer for the dynamic neural architectures. After that, Progressive Nets and Dynamic Expandable Nets(DEN) were developed.\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/expert_gate.png?raw=true' width=425 >\n",
        "\n",
        "\n",
        "Progressive Nets creates new nodes for each incoming task in addition to current nodes to overcome representation overlap. Besides, it freezes the previous weights to preserve the knowledge. DEN, on the other hand, expands its structure when it is needed since growing the network for each task is highly expensive. It also allows patrial retraining from the previous weights.\n",
        "\n",
        "In the dynamic neural architectures we need to configure the following points though:\n",
        "* When should the network add/expand?\n",
        "* When should the network stop adding/expanding?\n",
        "* How should the network add/expand (neuron-wise,layer-wise)?\n",
        "\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/den.png?raw=true' width=550 >\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jr8qEyf4JVN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Protocol of the Continual Learning\n",
        "\n",
        "### Split by Patterns\n",
        "Each experience is split into train-validation-test sets. Test sets are then utilized to evaluate within the experiences. It is a parallel split through the experiences.\n",
        "\n",
        "* *Growing Test Set*:  Considers only the test set of the current and previously encountered experiences. In other words, the number of test sets grows over time as the number of experiences increases by introducing new experiences the model. \n",
        "\n",
        "* *Fixed Test Set*: The number of test sets does not grows over time because it is already fixed to the total number of experiences. This approach makes the evaluation process independent from the as the number of experiences the model have face with. It usually gives more clear view on overall system performance and more common for the benchmarks.\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/testset.png?raw=true' width=500 >\n",
        "\n",
        "As an example, imagine continual learning task with an *N* number of experiences: In the *Growing Test Set*, if the model currently in 𝑖<sup>th</sup> experience, then it will be evaluated with 𝑖 number of test set which are collected from experiences that have been seen so far. In the *Fixed Test Set*, however, there will be *N* number of test set regardless of the number of experience the model has seen so far. That is why, it is expected to see decreasing accuracy scores in *Growing Test Set* while seeing increasing accuracy scores in the *Fixed Test Set*.\n",
        "\n",
        "### Split by Experiences\n",
        "Experiences are split into two as train and test sets. Model that have been trained with experiences in the train-set is evaluated with the experiences in the test-set. In that approach, main interest is the evalutation of models' ability to learn a never-seen new piece of data which measures the generalization performance. It is a vertical split through the experiences.\n",
        "\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/testsplits.png?raw=true' width=1000 >\n"
      ],
      "metadata": {
        "id": "7foL_Wk7Zcnp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Evaluate?\n",
        "\n",
        "Evaluation in continual learning is unfortunately more challenging since we have more than one approach to create an architecture that not only learns but also remembers. In rehearsal=based approach, we should check the original data amount, generated data amount, memory size and computational expense. Meanwhile, in regularization-based approach, we should consider the regularization strength (hyperparameters), memory expense and computational expense. In architecture-based approach, on the other hand, we should pay attention to number of parameters, number of models, expert heads, memory expense and computational expense. Besides those measures, we should also investigate how accurate the models are. However, final average accuracy seemed insufficient since it does not provide any information about task specific results. In other words, do we care about the overall performance or do we care\n",
        "about the performance at one point in the current time. That is why, the majority of continual or lifelong learning approaches will in fact report more than a single loss or a single accuracy and a particular version in the figure has become quite prominent. It is called *per task measures* and here, we have several measures the evaluate the models. \n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/per_task_measure.png?raw=true' width=800 >\n",
        "\n",
        "First of all, **ideal loss or ideal accuruacy** represents the performance we can achieve given a specific model giving a specific data set if we train on everything at once. So, it's the maximum we kind of can expect to get according to our modeling assumptions. It is also referred as *measure of achieveable baseline.* **A base loss or base accuracy** measures that if we train an initial task then we keep essentially monitoring the performance just on this initial task alone over time rather than just looking at the average. It is also referred as *measure of retention.* Similarly, we can measure only the new task with **new loss or new accuracy**. If we're lacking in terms of capacity or representational capacity it might be that we're simply unable to encode a new task. It is also referred as *measure the ability to encode new tasks*. And finally, to see how the average looks like we can use **all loss or all accuracy** which simply average up to present point up time. It is also referred as *measure of present overall performance*.\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/forgetting_intransigence.png?raw=true' width=350 >\n",
        "\n",
        "\n",
        "Similarly, **forgetting** measures the maximum knowledge gained about the task throughout the learning process in the past and the knowledge the model currently has about it. And, **intransigence** measures the inability of a model to learn new tasks. Since we wish to quantify the inability to learn, we compare to the standard classification model which has access to all the datasets at all times. Finally, we have **forward transfer** that measures the influence of a learned task on the future tasks while **backward transfer** measures the influence of a new task on the previous tasks, negative = forgetting and positive = retrospective improvement.\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/fwt_bwt.png?raw=true' width=550 >\n",
        "\n",
        "## Desidereta of Continual Learning\n",
        "\n",
        "<img src='https://github.com/muratonuryildirim/Tutorials/blob/master/images/continual_learning/desiderata_cl.png?raw=true' width=800 >\n",
        "\n"
      ],
      "metadata": {
        "id": "KJZ-EZZ5UxIt"
      }
    }
  ]
}
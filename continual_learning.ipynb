{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "continual_learning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk5EsWdKbq7Wk5QR6H2t1u"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Continual Learning\n",
        "\n",
        "According to the traditional machine learning approach, it is assumed that data is already and always available. Hence, the model could work nicely if it is trained by the data on-hand. Recently, scientists and researchers realize that it is not the case. They, unfortunately, found that artificial neural networks have a tendency to forget. They even illustrated that the networks forget previously learned information completely and abruptly upon learning new information which is called **catastrophic forgetting**.\n",
        "\n",
        "When the trained model is faced with an unfamiliar observation or task, it could confuse and lead to wrong answers. Therefore, it has been suggested that this static approach should be replaced with the more dynamic one so that machine learning models can adapt themselves to the new tasks or experiments.\n",
        "\n",
        "Continual Learning is the concept to learn a model for a large number of tasks sequentially without forgetting knowledge obtained from the preceding tasks, where the data in the old tasks are not available anymore during the training of the new ones.\n",
        "\n",
        "Tasks should have the following criteria to be continuous:\n",
        "\n",
        "* Data (and tasks) become available only during the time.\n",
        "* No access to previously encountered data.\n",
        "* Constant computational and memory resources (efficiency)\n",
        "* Incremental development of ever more complex knowledge and skills (scalability)\n",
        "\n",
        "There are other also other approaches that should not be confused with the continual learning to make the machine learning systems more dynamic and adaptive:\n",
        "\n",
        "* Multi-Task Learning\n",
        "* Meta-Learning / Learning to\n",
        "Learn\n",
        "* Transfer Learning & Domain\n",
        "Adaptation\n",
        "* Online / Streaming Learning \n",
        "\n",
        "To sum up, continual learning aims find a well-generalized model that remembers past concepts while learning the new concepts as well. It is not an easy job since the model should understand which biased information should be removed and which biased information should be preserved. Continual learning also assumes that there will be no access to previously encountered data and tasks. That assumption is important from two different aspects: First, in a continuous environment, the thing that is gone is gone. It can not exist again. Second, from the resource aspect, it is not possible to store all the previous information the model has encountered before. As an example, an average human consumes approximately 50 GB/s while observing stuff which makes ~30240 TB of data after only a week."
      ],
      "metadata": {
        "id": "sCJr1cAglFcJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Catastrophic Forgetting\n",
        "\n",
        "Forgetting is something that is not just related to machines but it is part of the very nature of humans and the process of learning. It is actually important to forget the biased information or all information acquired over time in terms of effectiveness and efficiency. Models cannot remember everything and even if it remembers them it is just a waste of resource to remember things that are not useful to reach a particular goal.\n",
        "\n",
        "However, the level of forgetting is what matters here. The model should forget the unnecessary to generalize well yet it should retain the key information to 'learn'. The term **catastrophic forgetting**  means losing information completely and abruptly upon learning new information mostly due to the *gradient descent*. That is because, in the traditional approach, the model tries to minimize the loss **on a given task** which can be completely different for another task. Hence, continual learning shifts its standpoint a bit and it suggests that the model should try to minimize the loss **over the entire stream of task** which is represented as L<sub>$s$</sub>.\n",
        "\n",
        "The interesting and funny point here is this: the model could work perfectly if all the data at hand were fed to the model at once, yet when we split the data into smaller pieces and feed them into the model as a stream of tasks, the model is starting to forget. It is very remarkable to see how such a small change affects the performance of the model. However, it is not possible to have all the data at hand at once in a real case scenario which shows the motivation behind the continual learning and the significance of the catastrophic forgetting."
      ],
      "metadata": {
        "id": "RaQ-fnfCognu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Shifts in ML\n",
        "\n",
        "In the traditional offline machine leraning, data and its distrubtuion is fixed while in the real world scanerio data is something dynamic and can change over time. To define these kind of changes researchers have come up with different terms:\n",
        "* Covariate Shift (shift happens in inputs): P(X) - same input returns same output but inputs are changed a bit.\n",
        "* Prior Probabilty Shift (shift happpens in output): P(y) - same input returns same output but frequency or the distribution of the output is changed a bit.\n",
        "* Concept Shift (shift happpens in output): P(y|X) - same input does not return same output anymore. Model should adapt itself - means retraining is necessary.\n",
        "\n",
        "The first two terms are classified under a virtual drift which is concerned by continual learning since the shifts we encounter is just a the result of a sample or task selection order (or bias). Therefore, in the virtual drift case, it should be assumed that all the previous examples or the previous observations are still valid and we need to accumulate knowledge over time. The other term is classified under a real drift which mostly investigated by online learning and AutoML. To sum up, one should be able to identify the type of shift so that the necessary action can be taken.\n",
        "\n",
        "<img src=\"https://github.com/muratonuryildirim/Tutorials/blob/master/images/datashift.png?raw=true\" width=700>\n",
        "\n"
      ],
      "metadata": {
        "id": "M40lWcEkePuz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Assumptions in Continual Learning\n",
        "\n",
        "* *Shift is only virtual* which means forgetting is not needed only accumulation of knowledge would be enough.\n",
        "* *No conflicting evidence* that stands for one x value can be only \n",
        "valid for one y since we are modeling a mathematical function.\n",
        "* *Unbounded time between two experiences* which describes multiple time of training can possible for one experiment.\n",
        "* *Data processing valid in each experience* so that data in a given experience can be shuffled and processed freely."
      ],
      "metadata": {
        "id": "c5Rasq6nqmId"
      }
    }
  ]
}